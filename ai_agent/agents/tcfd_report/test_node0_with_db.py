"""
Node 0 DB ì—°ê²° í…ŒìŠ¤íŠ¸ (Primary Data ê²€ì¦)
ì‹¤ì œ DBì—ì„œ ì‚¬ì—…ì¥ ë°ì´í„° ì¡°íšŒ ë° BC/AD Agent ì‹¤í–‰ í…ŒìŠ¤íŠ¸

ì‹¤í–‰ ë°©ë²•:
    cd polaris_backend_fastapi
    python ai_agent/agents/tcfd_report/test_node0_with_db.py

í™˜ê²½ë³€ìˆ˜ í•„ìš”:
    - APPLICATION_DATABASE_URL: SpringBoot DB (sites í…Œì´ë¸”)
    - DATABASE_URL: Datawarehouse DB (ModelOps ê²°ê³¼ í…Œì´ë¸”ë“¤)
    - OPENAI_API_KEY: LLM í´ë¼ì´ì–¸íŠ¸ìš©

í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤:
    1. Application DBì—ì„œ ì‚¬ì—…ì¥ ê¸°ë³¸ ì •ë³´ ì¡°íšŒ
    2. Datawarehouse DBì—ì„œ 5ê°œ ModelOps ê²°ê³¼ í…Œì´ë¸” ì¡°íšŒ
    3. BC Agent ì‹¤í–‰ (ê±´ì¶•ë¬¼ ë¶„ì„)
    4. AD Agent ì‹¤í–‰ (ì¶”ê°€ ë°ì´í„° ë¶„ì„)
    5. ìµœì¢… ë°˜í™˜ JSON êµ¬ì¡° ê²€ì¦
"""

import asyncio
import os
import sys
import json
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

# sys.pathì— í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from ai_agent.agents.tcfd_report.node_0_data_preprocessing import DataPreprocessingNode


# ===== í™˜ê²½ë³€ìˆ˜ ì²´í¬ =====

def check_environment():
    """í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ í™•ì¸"""
    required_env_vars = [
        'APPLICATION_DATABASE_URL',
        'DATABASE_URL',
        'OPENAI_API_KEY'
    ]

    missing_vars = []
    for var in required_env_vars:
        if not os.getenv(var):
            missing_vars.append(var)

    if missing_vars:
        print("âŒ í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤:")
        for var in missing_vars:
            print(f"   - {var}")
        print("\n.env íŒŒì¼ì„ í™•ì¸í•˜ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        sys.exit(1)

    print("âœ… í™˜ê²½ë³€ìˆ˜ í™•ì¸ ì™„ë£Œ\n")


# ===== í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ =====

async def test_node0_basic(site_ids: List[int]):
    """
    í…ŒìŠ¤íŠ¸ 1: Node 0 ê¸°ë³¸ ì‹¤í–‰
    - DB ì¡°íšŒë§Œ (target_years=None â†’ ì „ì²´ ë…„ë„)
    """
    print("=" * 80)
    print("í…ŒìŠ¤íŠ¸ 1: Node 0 ê¸°ë³¸ ì‹¤í–‰ (ì „ì²´ ë…„ë„)")
    print("=" * 80)
    print(f"Site IDs: {site_ids}\n")

    # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (BC/AD Agentìš©)
    from langchain_openai import ChatOpenAI
    llm_client = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.0,
        api_key=os.getenv("OPENAI_API_KEY")
    )

    # Node 0 ì´ˆê¸°í™”
    node = DataPreprocessingNode(
        app_db_url=os.getenv('APPLICATION_DATABASE_URL'),
        dw_db_url=os.getenv('DATABASE_URL'),
        llm_client=llm_client,
        max_concurrent_sites=10,
        bc_chunk_size=3
    )

    # ì‹¤í–‰
    print("â–¶ Node 0 ì‹¤í–‰ ì‹œì‘...\n")
    start_time = datetime.now()

    try:
        result = await node.execute(
            site_ids=site_ids,
            excel_file=None,  # DEPRECATED
            target_years=None  # ì „ì²´ ë…„ë„ ì¡°íšŒ
        )

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        print(f"\nâœ… Node 0 ì‹¤í–‰ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {duration:.2f}ì´ˆ)\n")

        # ê²°ê³¼ ê²€ì¦
        print("=" * 80)
        print("ğŸ“Š ê²°ê³¼ ê²€ì¦")
        print("=" * 80)

        # 1. sites_data ê²€ì¦
        sites_data = result.get("sites_data", [])
        print(f"âœ… sites_data: {len(sites_data)}ê°œ ì‚¬ì—…ì¥")
        for site in sites_data:
            site_info = site.get("site_info", {})
            print(f"   - site_id={site['site_id']}, name={site_info.get('name')}, "
                  f"lat={site_info.get('latitude')}, lon={site_info.get('longitude')}")

        # 2. ModelOps 5ê°œ í…Œì´ë¸” ê²°ê³¼ ê²€ì¦
        aal_results = result.get("aal_scaled_results", [])
        hazard_results = result.get("hazard_results", [])
        exposure_results = result.get("exposure_results", [])
        vulnerability_results = result.get("vulnerability_results", [])
        probability_results = result.get("probability_results", [])

        print(f"\nâœ… ModelOps ê²°ê³¼ í…Œì´ë¸”:")
        print(f"   - aal_scaled_results: {len(aal_results)}ê°œ row")
        print(f"   - hazard_results: {len(hazard_results)}ê°œ row")
        print(f"   - exposure_results: {len(exposure_results)}ê°œ row")
        print(f"   - vulnerability_results: {len(vulnerability_results)}ê°œ row")
        print(f"   - probability_results: {len(probability_results)}ê°œ row")

        # AAL ìƒ˜í”Œ ì¶œë ¥
        if aal_results:
            print(f"\n   [AAL Sample]")
            sample = aal_results[0]
            print(f"   site_id={sample.get('site_id')}, risk_type={sample.get('risk_type')}, "
                  f"target_year={sample.get('target_year')}")
            print(f"   SSP245 Final AAL: {sample.get('ssp245_final_aal')}")

        # 3. building_data ê²€ì¦ (BC Agent ê²°ê³¼)
        building_data = result.get("building_data", {})
        print(f"\nâœ… building_data (BC Agent): {len(building_data)}ê°œ ì‚¬ì—…ì¥")
        for site_id, bc_result in building_data.items():
            print(f"   - site_id={site_id}")
            print(f"     * structural_grade: {bc_result.get('structural_grade')}")
            print(f"     * has_raw_data: {bool(bc_result.get('raw_data'))}")

            # BC Agent guideline ìƒ˜í”Œ
            guideline = bc_result.get('guideline', '')
            if guideline:
                preview = guideline[:100] + "..." if len(guideline) > 100 else guideline
                print(f"     * guideline: {preview}")

        # 4. additional_data ê²€ì¦ (AD Agent ê²°ê³¼)
        additional_data = result.get("additional_data", {})
        use_additional_data = result.get("use_additional_data", False)

        print(f"\nâœ… additional_data (AD Agent): use_additional_data={use_additional_data}")
        if use_additional_data and additional_data:
            data_summary = additional_data.get('data_summary', {})
            print(f"   - data_summary.one_liner: {data_summary.get('one_liner', 'N/A')}")

            site_guidelines = additional_data.get('site_specific_guidelines', {})
            print(f"   - site_specific_guidelines: {len(site_guidelines)}ê°œ ì‚¬ì—…ì¥")

            raw_data = additional_data.get('raw_data', {})
            print(f"   - raw_data: {len(raw_data)}ê°œ ì‚¬ì—…ì¥")

        print("\n" + "=" * 80)
        print("ğŸ‰ í…ŒìŠ¤íŠ¸ 1 í†µê³¼!")
        print("=" * 80 + "\n")

        return result

    except Exception as e:
        print(f"\nâŒ Node 0 ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        raise


async def test_node0_filtered_years(site_ids: List[int]):
    """
    í…ŒìŠ¤íŠ¸ 2: Node 0 íŠ¹ì • ë…„ë„ë§Œ ì¡°íšŒ
    - target_years = ["2030", "2050s"]
    """
    print("=" * 80)
    print("í…ŒìŠ¤íŠ¸ 2: Node 0 íŠ¹ì • ë…„ë„ í•„í„°ë§")
    print("=" * 80)
    print(f"Site IDs: {site_ids}")
    print(f"Target Years: ['2030', '2050s']\n")

    from langchain_openai import ChatOpenAI
    llm_client = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.0,
        api_key=os.getenv("OPENAI_API_KEY")
    )

    node = DataPreprocessingNode(
        app_db_url=os.getenv('APPLICATION_DATABASE_URL'),
        dw_db_url=os.getenv('DATABASE_URL'),
        llm_client=llm_client
    )

    print("â–¶ Node 0 ì‹¤í–‰ ì‹œì‘ (ë…„ë„ í•„í„°ë§)...\n")
    start_time = datetime.now()

    try:
        result = await node.execute(
            site_ids=site_ids,
            excel_file=None,
            target_years=["2030", "2050s"]  # í•„í„°ë§
        )

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        print(f"\nâœ… Node 0 ì‹¤í–‰ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {duration:.2f}ì´ˆ)\n")

        # ê²°ê³¼ ê²€ì¦: target_yearê°€ 2030 ë˜ëŠ” 2050së§Œ ìˆì–´ì•¼ í•¨
        print("=" * 80)
        print("ğŸ“Š Year í•„í„°ë§ ê²€ì¦")
        print("=" * 80)

        aal_results = result.get("aal_scaled_results", [])
        unique_years = set(r.get('target_year') for r in aal_results)

        print(f"âœ… AAL Results: {len(aal_results)}ê°œ row")
        print(f"   Unique target_years: {sorted(unique_years)}")

        # ê²€ì¦: 2030, 2050së§Œ ìˆì–´ì•¼ í•¨
        expected_years = {"2030", "2050s"}
        if unique_years <= expected_years:
            print(f"   âœ… í•„í„°ë§ ì„±ê³µ! (expected: {expected_years})")
        else:
            print(f"   âŒ í•„í„°ë§ ì‹¤íŒ¨! unexpected years: {unique_years - expected_years}")

        print("\n" + "=" * 80)
        print("ğŸ‰ í…ŒìŠ¤íŠ¸ 2 í†µê³¼!")
        print("=" * 80 + "\n")

        return result

    except Exception as e:
        print(f"\nâŒ Node 0 ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        raise


async def test_node0_json_structure(site_ids: List[int]):
    """
    í…ŒìŠ¤íŠ¸ 3: Node 0 ë°˜í™˜ JSON êµ¬ì¡° ê²€ì¦
    - state.pyì˜ TCFDReportStateì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
    """
    print("=" * 80)
    print("í…ŒìŠ¤íŠ¸ 3: Node 0 ë°˜í™˜ JSON êµ¬ì¡° ê²€ì¦")
    print("=" * 80)

    from langchain_openai import ChatOpenAI
    llm_client = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.0,
        api_key=os.getenv("OPENAI_API_KEY")
    )

    node = DataPreprocessingNode(
        app_db_url=os.getenv('APPLICATION_DATABASE_URL'),
        dw_db_url=os.getenv('DATABASE_URL'),
        llm_client=llm_client
    )

    result = await node.execute(
        site_ids=site_ids,
        excel_file=None,
        target_years=["2030"]  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©
    )

    # í•„ë“œ ê²€ì¦
    print("\nğŸ“‹ ë°˜í™˜ í•„ë“œ ê²€ì¦:")

    required_fields = [
        "sites_data",
        "aal_scaled_results",
        "hazard_results",
        "exposure_results",
        "vulnerability_results",
        "probability_results",
        "building_data",
        "additional_data",
        "use_additional_data"
    ]

    missing_fields = []
    for field in required_fields:
        if field in result:
            value = result[field]
            value_type = type(value).__name__

            # ê°’ì˜ ê¸¸ì´ í™•ì¸ (list/dictì¸ ê²½ìš°)
            if isinstance(value, list):
                length_info = f"len={len(value)}"
            elif isinstance(value, dict):
                length_info = f"keys={len(value)}"
            elif isinstance(value, bool):
                length_info = f"value={value}"
            else:
                length_info = ""

            print(f"   âœ… {field}: {value_type} {length_info}")
        else:
            missing_fields.append(field)
            print(f"   âŒ {field}: MISSING")

    if missing_fields:
        print(f"\nâŒ í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {missing_fields}")
        raise AssertionError(f"Missing fields: {missing_fields}")

    # JSON ì§ë ¬í™” ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    print("\nğŸ“‹ JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸:")
    try:
        # building_dataì™€ additional_dataëŠ” ë³µì¡í•œ êµ¬ì¡°ì´ë¯€ë¡œ ìƒ˜í”Œë§Œ í…ŒìŠ¤íŠ¸
        test_result = {
            "sites_data": result["sites_data"],
            "aal_scaled_results": result["aal_scaled_results"][:5] if result["aal_scaled_results"] else [],
            "use_additional_data": result["use_additional_data"]
        }

        json_str = json.dumps(test_result, indent=2, default=str)
        print(f"   âœ… JSON ì§ë ¬í™” ì„±ê³µ (ìƒ˜í”Œ í¬ê¸°: {len(json_str)} bytes)")
    except Exception as e:
        print(f"   âŒ JSON ì§ë ¬í™” ì‹¤íŒ¨: {e}")
        raise

    print("\n" + "=" * 80)
    print("ğŸ‰ í…ŒìŠ¤íŠ¸ 3 í†µê³¼!")
    print("=" * 80 + "\n")

    return result


# ===== ë©”ì¸ ì‹¤í–‰ =====

async def main():
    """ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("\n" + "ğŸ§ª" * 40)
    print("Node 0 DB ì—°ê²° í…ŒìŠ¤íŠ¸ (Primary Data ê²€ì¦)")
    print("ğŸ§ª" * 40 + "\n")

    # í™˜ê²½ë³€ìˆ˜ ì²´í¬
    check_environment()

    # í…ŒìŠ¤íŠ¸í•  site_ids ì…ë ¥ë°›ê¸°
    print("=" * 80)
    print("ì‚¬ì—…ì¥ ID ì…ë ¥")
    print("=" * 80)
    print("í…ŒìŠ¤íŠ¸í•  ì‚¬ì—…ì¥ IDë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì‰¼í‘œë¡œ êµ¬ë¶„):")
    print("ì˜ˆ: 1,2,3")
    print()

    # ê¸°ë³¸ê°’ ì„¤ì • (ì…ë ¥ ì—†ìœ¼ë©´ ì‚¬ìš©)
    default_site_ids = [1]

    user_input = input("Site IDs (Enter = ê¸°ë³¸ê°’ ì‚¬ìš©): ").strip()

    if user_input:
        try:
            site_ids = [int(sid.strip()) for sid in user_input.split(",")]
        except ValueError:
            print("âŒ ì˜ëª»ëœ ì…ë ¥ í˜•ì‹ì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            site_ids = default_site_ids
    else:
        site_ids = default_site_ids

    print(f"\nâœ… Site IDs: {site_ids}\n")

    try:
        # í…ŒìŠ¤íŠ¸ 1: ê¸°ë³¸ ì‹¤í–‰ (ì „ì²´ ë…„ë„)
        result1 = await test_node0_basic(site_ids)

        # í…ŒìŠ¤íŠ¸ 2: ë…„ë„ í•„í„°ë§
        result2 = await test_node0_filtered_years(site_ids)

        # í…ŒìŠ¤íŠ¸ 3: JSON êµ¬ì¡° ê²€ì¦
        result3 = await test_node0_json_structure(site_ids)

        print("\n" + "=" * 80)
        print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        print("=" * 80)
        print("\nì£¼ìš” ê²€ì¦ í•­ëª©:")
        print("  âœ… Application DB ì‚¬ì—…ì¥ ì •ë³´ ì¡°íšŒ")
        print("  âœ… Datawarehouse DB ModelOps 5ê°œ í…Œì´ë¸” ì¡°íšŒ")
        print("  âœ… BC Agent ì‹¤í–‰ (ê±´ì¶•ë¬¼ ë¶„ì„)")
        print("  âœ… AD Agent ì‹¤í–‰ (ì¶”ê°€ ë°ì´í„° ë¶„ì„)")
        print("  âœ… target_years í•„í„°ë§")
        print("  âœ… ë°˜í™˜ JSON êµ¬ì¡° ê²€ì¦")
        print()

        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        print("=" * 80)
        print("ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½")
        print("=" * 80)
        print(f"âœ… ì‚¬ì—…ì¥ ìˆ˜: {len(result1.get('sites_data', []))}")
        print(f"âœ… AAL ê²°ê³¼: {len(result1.get('aal_scaled_results', []))} rows")
        print(f"âœ… BC Agent: {len(result1.get('building_data', {}))} sites")
        print(f"âœ… AD Agent: use_additional_data={result1.get('use_additional_data')}")
        print()

    except AssertionError as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise
    except Exception as e:
        print(f"\nâŒ ì˜ˆì™¸ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    asyncio.run(main())
