"""
íŒŒì¼ëª…: document_parser.py
ìµœì¢… ìˆ˜ì •ì¼: 2025-12-12
ë²„ì „: v01
íŒŒì¼ ê°œìš”: LlamaParseë¥¼ í™œìš©í•œ PDF ë¬¸ì„œ íŒŒì‹± ì„œë¹„ìŠ¤

ì£¼ì˜ì‚¬í•­:
- LlamaParse Free Tier: 1,000 pages/month
- íŒŒì‹± ê²°ê³¼ëŠ” ë¡œì»¬ ìºì‹œì— ì €ì¥í•˜ì—¬ ì¬ì‚¬ìš©
- ì‹¤ì œ íŒŒì‹± ì‹¤í–‰ì€ scripts/parse_rag_documents.pyì—ì„œë§Œ ìˆ˜í–‰
"""

import os
import json
from typing import List, Dict, Any, Optional
from pathlib import Path
from llama_parse import LlamaParse
from dotenv import load_dotenv


load_dotenv()


class DocumentParser:
    """
    LlamaParseë¥¼ í™œìš©í•œ PDF ë¬¸ì„œ íŒŒì‹± í´ë˜ìŠ¤

    ì£¼ìš” ê¸°ëŠ¥:
    - PDF â†’ Markdown ë³€í™˜
    - í…Œì´ë¸” êµ¬ì¡° ë³´ì¡´
    - íŒŒì‹± ê²°ê³¼ ìºì‹± (ì¬íŒŒì‹± ë°©ì§€)
    """

    def __init__(
        self,
        result_type: str = "markdown",
        verbose: bool = True,
        cache_dir: str = "data/parsed_docs"
    ):
        """
        Args:
            result_type: íŒŒì‹± ê²°ê³¼ í˜•ì‹ ("text", "markdown", "json")
            verbose: íŒŒì‹± ì§„í–‰ ìƒí™© ë¡œê¹… ì—¬ë¶€
            cache_dir: íŒŒì‹± ê²°ê³¼ ìºì‹œ ë””ë ‰í† ë¦¬
        """
        api_key = os.getenv("LLAMA_CLOUD_API_KEY")
        if not api_key:
            raise ValueError(
                "LLAMA_CLOUD_API_KEY not found in environment variables. "
                "Please add it to .env file."
            )

        self.parser = LlamaParse(
            api_key=api_key,
            result_type=result_type,
            verbose=verbose
        )

        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # íŒŒì‹± ë©”íƒ€ë°ì´í„° íŒŒì¼
        self.metadata_file = self.cache_dir / "parsing_metadata.json"
        self._init_metadata()

    def _init_metadata(self):
        """ë©”íƒ€ë°ì´í„° íŒŒì¼ ì´ˆê¸°í™”"""
        if not self.metadata_file.exists():
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "parsed_documents": [],
                    "total_pages_used": 0,
                    "last_updated": None
                }, f, indent=2)

    def _load_metadata(self) -> Dict[str, Any]:
        """ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        with open(self.metadata_file, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _save_metadata(self, metadata: Dict[str, Any]):
        """ë©”íƒ€ë°ì´í„° ì €ì¥"""
        with open(self.metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

    def _get_cache_path(self, file_path: str) -> Path:
        """íŒŒì¼ ê²½ë¡œì—ì„œ ìºì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„±"""
        file_name = Path(file_path).stem
        return self.cache_dir / f"{file_name}_parsed.json"

    def _save_to_cache(
        self,
        documents: List[Any],
        cache_path: Path,
        source_file: str,
        page_count: int
    ):
        """íŒŒì‹± ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥"""
        cache_data = {
            "source_file": source_file,
            "page_count": page_count,
            "documents": [
                {
                    "text": doc.text,
                    "metadata": doc.metadata
                }
                for doc in documents
            ]
        }

        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, indent=2, ensure_ascii=False)

        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        metadata = self._load_metadata()
        metadata["parsed_documents"].append({
            "source_file": source_file,
            "cache_path": str(cache_path),
            "page_count": page_count,
            "parsed_at": str(Path().absolute())
        })
        metadata["total_pages_used"] += page_count

        from datetime import datetime
        metadata["last_updated"] = datetime.now().isoformat()

        self._save_metadata(metadata)

        print(f"âœ… Cached to: {cache_path}")
        print(f"ğŸ“Š Total pages used: {metadata['total_pages_used']} / 1,000 (Free Tier)")

    def _load_from_cache(self, cache_path: Path) -> List[Dict[str, Any]]:
        """ìºì‹œì—ì„œ íŒŒì‹± ê²°ê³¼ ë¡œë“œ"""
        if not cache_path.exists():
            return None

        with open(cache_path, 'r', encoding='utf-8') as f:
            cache_data = json.load(f)

        print(f"âœ… Loaded from cache: {cache_path}")
        return cache_data["documents"]

    def parse_pdf(
        self,
        file_path: str,
        force_reparse: bool = False
    ) -> List[Dict[str, Any]]:
        """
        PDF íŒŒì‹± (ìºì‹œ ìš°ì„  ì‚¬ìš©)

        Args:
            file_path: PDF íŒŒì¼ ê²½ë¡œ
            force_reparse: Trueë©´ ìºì‹œ ë¬´ì‹œí•˜ê³  ì¬íŒŒì‹± (ì£¼ì˜: ì‚¬ìš©ëŸ‰ ì†Œì§„)

        Returns:
            íŒŒì‹±ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not Path(file_path).exists():
            raise FileNotFoundError(f"File not found: {file_path}")

        cache_path = self._get_cache_path(file_path)

        # ìºì‹œ í™•ì¸
        if not force_reparse:
            cached_docs = self._load_from_cache(cache_path)
            if cached_docs:
                return cached_docs

        # ì‹¤ì œ íŒŒì‹± ì‹¤í–‰ (ì£¼ì˜: LlamaParse API í˜¸ì¶œ)
        print(f"âš ï¸  Parsing {file_path} using LlamaParse...")
        print(f"âš ï¸  This will consume Free Tier quota.")

        documents = self.parser.load_data(file_path)

        # í˜ì´ì§€ ìˆ˜ ì¶”ì • (ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ì—ì„œ í™•ì¸)
        page_count = self._estimate_page_count(documents)

        # ìºì‹œ ì €ì¥
        self._save_to_cache(documents, cache_path, file_path, page_count)

        return [
            {
                "text": doc.text,
                "metadata": doc.metadata
            }
            for doc in documents
        ]

    def _estimate_page_count(self, documents: List[Any]) -> int:
        """íŒŒì‹±ëœ ë¬¸ì„œì—ì„œ í˜ì´ì§€ ìˆ˜ ì¶”ì •"""
        # LlamaParseëŠ” ë©”íƒ€ë°ì´í„°ì— í˜ì´ì§€ ì •ë³´ í¬í•¨
        pages = set()
        for doc in documents:
            if hasattr(doc, 'metadata') and 'page' in doc.metadata:
                pages.add(doc.metadata['page'])

        return len(pages) if pages else len(documents)

    def extract_tables_from_markdown(
        self,
        markdown_text: str
    ) -> List[Dict[str, Any]]:
        """
        Markdown í…ìŠ¤íŠ¸ì—ì„œ í…Œì´ë¸” ì¶”ì¶œ

        Args:
            markdown_text: Markdown í˜•ì‹ í…ìŠ¤íŠ¸

        Returns:
            í…Œì´ë¸” ë¦¬ìŠ¤íŠ¸ [{"markdown": "...", "data": {...}}]
        """
        import re

        tables = []

        # Markdown í…Œì´ë¸” íŒ¨í„´: | header | header |
        table_pattern = r'(\|[^\n]+\|\n\|[-:\s|]+\|\n(?:\|[^\n]+\|\n)+)'

        matches = re.finditer(table_pattern, markdown_text, re.MULTILINE)

        for match in matches:
            table_markdown = match.group(0)

            # í…Œì´ë¸” ë°ì´í„° íŒŒì‹±
            lines = table_markdown.strip().split('\n')
            if len(lines) < 3:  # header + separator + at least 1 row
                continue

            # Header ì¶”ì¶œ
            headers = [cell.strip() for cell in lines[0].split('|') if cell.strip()]

            # Rows ì¶”ì¶œ (separator ì œì™¸)
            rows = []
            for line in lines[2:]:
                cells = [cell.strip() for cell in line.split('|') if cell.strip()]
                if len(cells) == len(headers):
                    rows.append(cells)

            tables.append({
                "markdown": table_markdown,
                "data": {
                    "headers": headers,
                    "rows": rows
                }
            })

        return tables

    def get_parsing_stats(self) -> Dict[str, Any]:
        """íŒŒì‹± í†µê³„ ë°˜í™˜"""
        metadata = self._load_metadata()

        return {
            "total_documents_parsed": len(metadata["parsed_documents"]),
            "total_pages_used": metadata["total_pages_used"],
            "free_tier_limit": 1000,
            "remaining_pages": 1000 - metadata["total_pages_used"],
            "last_updated": metadata["last_updated"],
            "documents": metadata["parsed_documents"]
        }


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    parser = DocumentParser()

    # í†µê³„ í™•ì¸
    stats = parser.get_parsing_stats()
    print("\nğŸ“Š Parsing Statistics:")
    print(json.dumps(stats, indent=2, ensure_ascii=False))

    # íŒŒì‹± ì˜ˆì‹œ (ì£¼ì˜: ì‹¤ì œë¡œ ì‹¤í–‰í•˜ì§€ ë§ ê²ƒ!)
    # documents = parser.parse_pdf("path/to/document.pdf")
