"""
ì „ì²´ í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
primary_data ì—ì´ì „íŠ¸ â†’ tcfd_report Node 0 í…ŒìŠ¤íŠ¸

ì‹¤í–‰: uv run python test_full_flow.py
"""

import os
import sys
import asyncio
import logging
from pathlib import Path
from decimal import Decimal

import psycopg2
from psycopg2.extras import RealDictCursor, Json
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

load_dotenv(project_root / ".env")

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("test_full_flow")


# ============================================================
# DB ì„¤ì •
# ============================================================

DB_CONFIG = {
    "host": "localhost",
    "port": "5555",
    "dbname": "postgres",
    "user": "skala",
    "password": "skala1234"
}


def get_db_connection():
    return psycopg2.connect(**DB_CONFIG)


# ============================================================
# ëª©ë°ì´í„° ì •ì˜
# ============================================================

MOCK_SITES = [
    {
        "id": "11111111-1111-1111-1111-111111111111",
        "user_id": "aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa",
        "name": "í…ŒìŠ¤íŠ¸ ì„œìš¸ë³¸ì‚¬",
        "road_address": "ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123",
        "jibun_address": "ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ ì—­ì‚¼ë™ 123-45",
        "latitude": Decimal("37.501200"),
        "longitude": Decimal("127.039600"),
        "type": "office"
    },
    {
        "id": "22222222-2222-2222-2222-222222222222",
        "user_id": "aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa",
        "name": "í…ŒìŠ¤íŠ¸ ë¶€ì‚°ê³µì¥",
        "road_address": "ë¶€ì‚°ê´‘ì—­ì‹œ ê°•ì„œêµ¬ ë…¹ì‚°ì‚°ì—…ì¤‘ë¡œ 456",
        "jibun_address": "ë¶€ì‚°ê´‘ì—­ì‹œ ê°•ì„œêµ¬ ì†¡ì •ë™ 456-78",
        "latitude": Decimal("35.089400"),
        "longitude": Decimal("128.853900"),
        "type": "factory"
    }
]

RISK_TYPES = ["TYPHOON", "INLAND_FLOOD", "HEAT_WAVE", "DROUGHT"]
TARGET_YEARS = ["2030", "2050s"]


def insert_mock_data():
    """ëª©ë°ì´í„° ì‚½ì… (í…Œì´ë¸” ìƒì„± X)"""
    logger.info("=== ëª©ë°ì´í„° ì‚½ì… ===")

    conn = get_db_connection()
    cursor = conn.cursor()

    for site in MOCK_SITES:
        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
        cursor.execute("DELETE FROM sites WHERE id = %s", (site["id"],))
        cursor.execute("DELETE FROM aal_scaled_results WHERE site_id = %s", (site["id"],))
        cursor.execute("DELETE FROM exposure_results WHERE site_id = %s", (site["id"],))
        cursor.execute("DELETE FROM vulnerability_results WHERE site_id = %s", (site["id"],))
        cursor.execute(
            "DELETE FROM hazard_results WHERE latitude = %s AND longitude = %s",
            (site["latitude"], site["longitude"])
        )
        cursor.execute(
            "DELETE FROM probability_results WHERE latitude = %s AND longitude = %s",
            (site["latitude"], site["longitude"])
        )

        # sites ì‚½ì…
        cursor.execute("""
            INSERT INTO sites (id, user_id, name, road_address, jibun_address, latitude, longitude, type)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
        """, (
            site["id"], site["user_id"], site["name"],
            site["road_address"], site["jibun_address"],
            site["latitude"], site["longitude"], site["type"]
        ))
        logger.info(f"  sites ì‚½ì…: {site['name']}")

        # 5ê°œ ê²°ê³¼ í…Œì´ë¸” ì‚½ì…
        for risk_type in RISK_TYPES:
            for target_year in TARGET_YEARS:
                # hazard_results
                cursor.execute("""
                    INSERT INTO hazard_results
                    (latitude, longitude, risk_type, target_year,
                     ssp126_score_100, ssp245_score_100, ssp370_score_100, ssp585_score_100)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                """, (
                    site["latitude"], site["longitude"], risk_type, target_year,
                    30.0, 40.0, 50.0, 60.0
                ))

                # probability_results
                cursor.execute("""
                    INSERT INTO probability_results
                    (latitude, longitude, risk_type, target_year,
                     ssp126_aal, ssp245_aal, ssp370_aal, ssp585_aal,
                     damage_rates, ssp126_bin_probs, ssp245_bin_probs, ssp370_bin_probs, ssp585_bin_probs)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                """, (
                    site["latitude"], site["longitude"], risk_type, target_year,
                    0.01, 0.02, 0.03, 0.04,
                    Json([0, 0.02, 0.07, 0.15, 0.30]),
                    Json([0.65, 0.20, 0.10, 0.04, 0.01]),
                    Json([0.60, 0.22, 0.12, 0.05, 0.01]),
                    Json([0.55, 0.24, 0.14, 0.05, 0.02]),
                    Json([0.50, 0.26, 0.16, 0.06, 0.02])
                ))

                # exposure_results
                cursor.execute("""
                    INSERT INTO exposure_results
                    (site_id, latitude, longitude, risk_type, target_year, exposure_score)
                    VALUES (%s, %s, %s, %s, %s, %s)
                """, (
                    site["id"], site["latitude"], site["longitude"],
                    risk_type, target_year, 55.0
                ))

                # vulnerability_results
                cursor.execute("""
                    INSERT INTO vulnerability_results
                    (site_id, latitude, longitude, risk_type, target_year, vulnerability_score)
                    VALUES (%s, %s, %s, %s, %s, %s)
                """, (
                    site["id"], site["latitude"], site["longitude"],
                    risk_type, target_year, 45.0
                ))

                # aal_scaled_results
                cursor.execute("""
                    INSERT INTO aal_scaled_results
                    (site_id, latitude, longitude, risk_type, target_year,
                     ssp126_final_aal, ssp245_final_aal, ssp370_final_aal, ssp585_final_aal)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                """, (
                    site["id"], site["latitude"], site["longitude"],
                    risk_type, target_year,
                    0.005, 0.008, 0.012, 0.018
                ))

    conn.commit()
    cursor.close()
    conn.close()

    logger.info("ëª©ë°ì´í„° ì‚½ì… ì™„ë£Œ!")
    return [site["id"] for site in MOCK_SITES]


# ============================================================
# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
# ============================================================

async def test_primary_data_agents():
    """Step 1: Primary Data ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸"""
    logger.info("\n" + "=" * 60)
    logger.info("Step 1: Primary Data ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸")
    logger.info("=" * 60)

    # ì§ì ‘ import (tcfd_report __init__.py ìš°íšŒ)
    from ai_agent.agents.primary_data.building_characteristics_agent import BuildingCharacteristicsAgent

    bc_agent = BuildingCharacteristicsAgent(llm_client=None)

    test_site = {
        "site_id": 1,
        "site_info": {
            "latitude": 37.501200,
            "longitude": 127.039600,
            "address": "ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123"
        },
        "risk_results": [
            {"risk_type": "TYPHOON", "final_aal": 0.008, "physical_risk_score": 40.0},
        ]
    }

    logger.info("BuildingCharacteristicsAgent ë°°ì¹˜ ë¶„ì„ ì‹œì‘...")
    bc_results = await bc_agent.analyze_batch([test_site])

    for site_id, result in bc_results.items():
        logger.info(f"  ì‚¬ì—…ì¥ {site_id}:")
        logger.info(f"    - êµ¬ì¡° ë“±ê¸‰: {result.get('structural_grade', 'Unknown')}")
        logger.info(f"    - ì·¨ì•½ì  ìˆ˜: {len(result.get('vulnerabilities', []))}")
        logger.info(f"    - íšŒë³µë ¥ ìš”ì¸ ìˆ˜: {len(result.get('resilience', []))}")

    logger.info("âœ… Primary Data ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    return bc_results


async def test_node_0(site_ids):
    """Step 2: Node 0 (Data Preprocessing) í…ŒìŠ¤íŠ¸"""
    logger.info("\n" + "=" * 60)
    logger.info("Step 2: Node 0 (Data Preprocessing) í…ŒìŠ¤íŠ¸")
    logger.info("=" * 60)

    # ì§ì ‘ import
    from ai_agent.agents.tcfd_report.node_0_data_preprocessing import DataPreprocessingNode
    from langchain_openai import ChatOpenAI

    # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    openai_api_key = os.getenv("OPENAI_API_KEY")
    llm_client = None
    if openai_api_key:
        try:
            llm_client = ChatOpenAI(
                model="gpt-4o-mini",
                temperature=0.3,
                api_key=openai_api_key
            )
            logger.info("âœ… LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ (gpt-4o-mini)")
        except Exception as e:
            logger.warning(f"âš ï¸ LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    else:
        logger.warning("âš ï¸ OPENAI_API_KEY ì—†ìŒ - LLM ë¶„ì„ ê±´ë„ˆëœ€")

    # application DB: sites í…Œì´ë¸”ì´ ìˆëŠ” postgres DB
    # datawarehouse DB: building_aggregate_cache ë“± ê²°ê³¼ í…Œì´ë¸”ì´ ìˆëŠ” datawarehouse DB
    app_db_url = "postgresql://skala:skala1234@localhost:5555/postgres"
    dw_db_url = "postgresql://skala:skala1234@localhost:5555/datawarehouse"

    node = DataPreprocessingNode(
        app_db_url=app_db_url,
        dw_db_url=dw_db_url,
        llm_client=llm_client,
        max_concurrent_sites=5,
        bc_chunk_size=5
    )

    logger.info(f"Node 0 ì‹¤í–‰ ì‹œì‘... (site_ids: {site_ids})")

    try:
        state = await node.execute(
            site_ids=site_ids,
            excel_file=None,
            target_years=["2030", "2050s"]
        )

        logger.info("\n=== Node 0 ê²°ê³¼ ìš”ì•½ ===")
        logger.info(f"site_data: {len(state.get('site_data', []))}ê°œ ì‚¬ì—…ì¥")
        logger.info(f"hazard_results: {len(state.get('hazard_results', []))}ê°œ ë ˆì½”ë“œ")
        logger.info(f"exposure_results: {len(state.get('exposure_results', []))}ê°œ ë ˆì½”ë“œ")
        logger.info(f"vulnerability_results: {len(state.get('vulnerability_results', []))}ê°œ ë ˆì½”ë“œ")
        logger.info(f"aal_scaled_results: {len(state.get('aal_scaled_results', []))}ê°œ ë ˆì½”ë“œ")
        logger.info(f"probability_results: {len(state.get('probability_results', []))}ê°œ ë ˆì½”ë“œ")
        logger.info(f"building_data: {len(state.get('building_data', {}))}ê°œ ì‚¬ì—…ì¥")
        logger.info(f"use_additional_data: {state.get('use_additional_data', False)}")

        # building_data ìƒì„¸ ë‚´ìš© ì¶œë ¥ (primary_data ì—ì´ì „íŠ¸ ê²°ê³¼ í™•ì¸)
        logger.info("\n=== Primary Data ì—ì´ì „íŠ¸ ê²°ê³¼ ìƒì„¸ ===")
        building_data = state.get('building_data', {})
        for site_id, bd in building_data.items():
            logger.info(f"\nğŸ“ ì‚¬ì—…ì¥: {site_id}")
            logger.info(f"  - êµ¬ì¡° ë“±ê¸‰: {bd.get('structural_grade', 'N/A')}")
            logger.info(f"  - ì·¨ì•½ì  ìˆ˜: {len(bd.get('vulnerabilities', []))}")
            for v in bd.get('vulnerabilities', [])[:3]:  # ìµœëŒ€ 3ê°œë§Œ
                logger.info(f"    â€¢ {v.get('factor', 'N/A')}: {v.get('severity', 'N/A')}")
            logger.info(f"  - íšŒë³µë ¥ ìš”ì¸ ìˆ˜: {len(bd.get('resilience', []))}")
            for r in bd.get('resilience', [])[:3]:
                logger.info(f"    â€¢ {r.get('factor', 'N/A')}: {r.get('strength', 'N/A')}")
            # agent_guidelines í™•ì¸ (LLM ë¶„ì„ ê²°ê³¼)
            guidelines = bd.get('agent_guidelines', {})
            if guidelines:
                if isinstance(guidelines, str):
                    logger.info(f"  - LLM ê°€ì´ë“œë¼ì¸: {len(guidelines)}ì í…ìŠ¤íŠ¸")
                    logger.info(f"    â€¢ {guidelines[:200]}...")
                elif isinstance(guidelines, dict):
                    logger.info(f"  - LLM ê°€ì´ë“œë¼ì¸: {len(guidelines)}ê°œ í•­ëª©")
                    for key in list(guidelines.keys())[:3]:
                        val = guidelines[key]
                        if isinstance(val, str) and len(val) > 100:
                            val = val[:100] + "..."
                        logger.info(f"    â€¢ {key}: {val}")
            else:
                logger.info(f"  - LLM ê°€ì´ë“œë¼ì¸: ì—†ìŒ (LLM í´ë¼ì´ì–¸íŠ¸ ë¯¸ì„¤ì •)")

        logger.info("âœ… Node 0 í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return state

    except Exception as e:
        logger.error(f"âŒ Node 0 ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None


async def main():
    """ë©”ì¸ ì‹¤í–‰"""
    logger.info("\n" + "=" * 60)
    logger.info("ì „ì²´ í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info("í”Œë¡œìš°: primary_data ì—ì´ì „íŠ¸ â†’ tcfd_report Node 0")
    logger.info("=" * 60)

    # 1. ëª©ë°ì´í„° ì‚½ì…
    site_ids = insert_mock_data()

    # 2. Primary Data ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸
    await test_primary_data_agents()

    # 3. Node 0 í…ŒìŠ¤íŠ¸ (primary_data ì—ì´ì „íŠ¸ í¬í•¨)
    state = await test_node_0(site_ids)

    logger.info("\n" + "=" * 60)
    if state and state.get('site_data'):
        logger.info("âœ… ì „ì²´ í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
    else:
        logger.info("âŒ ì „ì²´ í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!")
    logger.info("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
