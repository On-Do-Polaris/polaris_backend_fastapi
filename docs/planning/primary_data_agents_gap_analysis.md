# Primary Data Agents - Gap Analysis & Production Deployment Plan

**ì‘ì„±ì¼**: 2025-12-15
**ë²„ì „**: v1.0
**ëŒ€ìƒ**: AdditionalDataAgent, BuildingCharacteristicsAgent
**ëª©ì **: í…ŒìŠ¤íŠ¸ ë‹¨ê³„ì—ì„œ í™•ì¸í•˜ì§€ ëª»í•œ ë¶€ë¶„ íŒŒì•… ë° ì‹¤ì œ ì‹œìŠ¤í…œ ì ìš©ì„ ìœ„í•œ ìˆ˜ì •ì‚¬í•­ ë¬¸ì„œí™”

---

## ğŸ“‹ Executive Summary

### âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ í•­ëª©

1. **ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥**
   - âœ… asyncio.gather() ê¸°ë°˜ ë³‘ë ¬ ì²˜ë¦¬ êµ¬í˜„
   - âœ… 5ê°œ ì‚¬ì—…ì¥ ë™ì‹œ ì²˜ë¦¬: 10.47ì´ˆ (ìˆœì°¨ ëŒ€ë¹„ 4.8x ì„±ëŠ¥ í–¥ìƒ)
   - âœ… ì‚¬ì—…ì¥ë‹¹ í‰ê·  ì²˜ë¦¬ ì‹œê°„: 2.09ì´ˆ

2. **Realistic Excel íŒŒì¼ ì²˜ë¦¬**
   - âœ… ìˆ«ì ì „ìš© ì‹œê³„ì—´ ë°ì´í„° (í—¤ë” ì œì™¸)
   - âœ… ì „ë ¥ ë°ì´í„°: ì‹œê°„(timestamp), ì‚¬ìš©ëŸ‰(kWh), ë¹„ìš©(ì›)
   - âœ… í™˜ê²½ ë°ì´í„°: ì‹œê°„(timestamp), ì˜¨ë„(Â°C), ìŠµë„(%), CO2(ppm)
   - âœ… 720í–‰ Ã— 5ê°œ ì‚¬ì—…ì¥ = 3,600ê°œ ë°ì´í„° í¬ì¸íŠ¸ ì²˜ë¦¬

3. **LLM ê°€ì´ë“œë¼ì¸ ìƒì„±**
   - âœ… ì‚¬ì—…ì¥ë³„ ê°€ì´ë“œë¼ì¸ ìƒì„± (í‰ê·  888ì)
   - âœ… Key Insights ì¶”ì¶œ (í‰ê·  4-5ê°œ/ì‚¬ì—…ì¥)
   - âœ… JSON ì§ë ¬í™” ê²€ì¦

4. **Scratch í´ë” êµ¬ì¡°**
   - âœ… `scratch/{site_id}/additional_data.xlsx` êµ¬ì¡° ê²€ì¦
   - âœ… 1 Excel = 1 ì‚¬ì—…ì¥ ì›ì¹™ í™•ì¸

### âš ï¸ ë¯¸ê²€ì¦ í•­ëª© (Production Gaps)

**Critical (í•„ìˆ˜)**:
1. ì‹¤ì œ DB ì—°ë™ (í˜„ì¬: Mock Excel íŒŒì¼)
2. TTL ê¸°ë°˜ Scratch í´ë” ìë™ ì •ë¦¬
3. ì•…ì„± Excel íŒŒì¼ ì²˜ë¦¬ (XSS, ëŒ€ìš©ëŸ‰ íŒŒì¼ ë“±)
4. ë„¤íŠ¸ì›Œí¬ ì¥ì•  ì‹œ ì¬ì‹œë„ ë¡œì§
5. ëŒ€ê·œëª¨ ì²˜ë¦¬ (100+ ì‚¬ì—…ì¥)

**Important (ì¤‘ìš”)**:
6. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
7. LLM API Rate Limiting
8. ë™ì‹œì„± Race Condition
9. Excel ì—…ë¡œë“œ í”„ë¡œì„¸ìŠ¤
10. ì „ì²´ Workflow í†µí•© í…ŒìŠ¤íŠ¸

**Nice-to-have (ê°œì„ )**:
11. ì¸ì¦/ê¶Œí•œ ê²€ì¦
12. ê°ì‚¬ ë¡œê·¸ (Audit Trail)
13. ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§
14. A/B í…ŒìŠ¤íŠ¸ (í”„ë¡¬í”„íŠ¸ ìµœì í™”)

---

## ğŸ” ìƒì„¸ Gap Analysis

### 1. Database Integration (Critical)

**í˜„ì¬ ìƒíƒœ**:
- í…ŒìŠ¤íŠ¸ì—ì„œëŠ” ë¯¸ë¦¬ ìƒì„±ëœ Excel íŒŒì¼ ì‚¬ìš©
- `scratch/{site_id}/additional_data.xlsx` ê²½ë¡œë¥¼ ì§ì ‘ ì§€ì •

**Production ìš”êµ¬ì‚¬í•­**:
1. **Site ID ì¡°íšŒ**: DBì—ì„œ ì‚¬ìš©ìê°€ ìš”ì²­í•œ site_ids ìœ íš¨ì„± ê²€ì¦
2. **Excel íŒŒì¼ ê²½ë¡œ ë§¤í•‘**: DB ë˜ëŠ” File Storageì—ì„œ ì‹¤ì œ Excel íŒŒì¼ ìœ„ì¹˜ ì¡°íšŒ
3. **ë©”íƒ€ë°ì´í„° ì €ì¥**: ë¶„ì„ ê²°ê³¼ë¥¼ DBì— ì €ì¥ (guideline, key_insights, analyzed_at)

**í•„ìš”í•œ ìˆ˜ì •**:

#### A. `additional_data_agent.py` - íŒŒì¼ ê²½ë¡œ ì¡°íšŒ ë¡œì§ ì¶”ê°€

```python
async def analyze_from_db(self, db_session, site_ids: List[int]) -> Dict[str, Any]:
    """
    DBì—ì„œ Excel íŒŒì¼ ê²½ë¡œ ì¡°íšŒ í›„ ë¶„ì„

    :param db_session: DB ì„¸ì…˜ (SQLAlchemy AsyncSession)
    :param site_ids: ì‚¬ì—…ì¥ ID ë¦¬ìŠ¤íŠ¸
    :return: ë¶„ì„ ê²°ê³¼
    """
    # 1. DBì—ì„œ site_ids ìœ íš¨ì„± ê²€ì¦
    from ai_agent.utils.building_data_fetcher import BuildingDataFetcher
    fetcher = BuildingDataFetcher()

    valid_sites = await fetcher.validate_site_ids(db_session, site_ids)
    if not valid_sites:
        raise ValueError(f"ìœ íš¨í•˜ì§€ ì•Šì€ site_ids: {site_ids}")

    # 2. Scratch í´ë”ì—ì„œ Excel íŒŒì¼ ê²½ë¡œ ì¡°íšŒ
    excel_files = {}
    for site_id in valid_sites:
        file_path = f"./scratch/{site_id}/additional_data.xlsx"
        if os.path.exists(file_path):
            excel_files[site_id] = file_path
        else:
            self.logger.warning(f"Site {site_id}: Excel íŒŒì¼ ì—†ìŒ ({file_path})")

    # 3. ê° ì‚¬ì—…ì¥ë³„ë¡œ ë³‘ë ¬ ë¶„ì„
    tasks = [
        self.analyze(excel_path, site_ids=[site_id])
        for site_id, excel_path in excel_files.items()
    ]

    results = await asyncio.gather(*tasks, return_exceptions=True)

    # 4. ê²°ê³¼ ë³‘í•©
    merged_guidelines = {}
    for site_id, result in zip(excel_files.keys(), results):
        if isinstance(result, Exception):
            self.logger.error(f"Site {site_id} ë¶„ì„ ì‹¤íŒ¨: {result}")
            continue

        site_guidelines = result.get('site_specific_guidelines', {})
        merged_guidelines.update(site_guidelines)

    # 5. DBì— ê²°ê³¼ ì €ì¥ (optional)
    await self._save_results_to_db(db_session, merged_guidelines)

    return {
        "meta": {
            "analyzed_at": datetime.now().isoformat(),
            "site_count": len(merged_guidelines),
            "total_sites_requested": len(site_ids)
        },
        "site_specific_guidelines": merged_guidelines,
        "status": "completed"
    }

async def _save_results_to_db(self, db_session, guidelines: Dict[int, Dict]):
    """ë¶„ì„ ê²°ê³¼ë¥¼ DBì— ì €ì¥"""
    # TODO: DB ìŠ¤í‚¤ë§ˆ ì„¤ê³„ í•„ìš”
    # ì˜ˆìƒ í…Œì´ë¸”: additional_data_analysis_results
    # ì»¬ëŸ¼: id, site_id, guideline, key_insights, analyzed_at
    pass
```

#### B. DB ìŠ¤í‚¤ë§ˆ ì„¤ê³„

```sql
-- ì¶”ê°€ ë°ì´í„° ë¶„ì„ ê²°ê³¼ í…Œì´ë¸”
CREATE TABLE additional_data_analysis_results (
    id SERIAL PRIMARY KEY,
    site_id INTEGER NOT NULL REFERENCES sites(id),
    analysis_date TIMESTAMP NOT NULL DEFAULT NOW(),
    guideline_text TEXT,
    key_insights JSONB,  -- Array of insights
    source_file_path VARCHAR(500),
    analyzed_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT NOW(),

    -- ì¸ë±ìŠ¤
    INDEX idx_site_analysis (site_id, analysis_date DESC)
);

-- Excel íŒŒì¼ ë©”íƒ€ë°ì´í„° í…Œì´ë¸” (íŒŒì¼ ê²½ë¡œ ê´€ë¦¬)
CREATE TABLE site_excel_files (
    id SERIAL PRIMARY KEY,
    site_id INTEGER NOT NULL REFERENCES sites(id),
    file_type VARCHAR(50),  -- 'power_data', 'environmental_data', etc.
    file_path VARCHAR(500) NOT NULL,  -- scratch/{site_id}/additional_data.xlsx
    uploaded_at TIMESTAMP NOT NULL,
    expires_at TIMESTAMP,  -- TTL for scratch cleanup
    file_size_bytes BIGINT,
    row_count INTEGER,

    -- ì¸ë±ìŠ¤
    INDEX idx_site_files (site_id, file_type),
    INDEX idx_ttl (expires_at) WHERE expires_at IS NOT NULL
);
```

---

### 2. TTL-based Scratch Folder Cleanup (Critical)

**í˜„ì¬ ìƒíƒœ**:
- Scratch í´ë”ëŠ” í…ŒìŠ¤íŠ¸ ì¢…ë£Œ ì‹œ ìˆ˜ë™ ì‚­ì œ
- TTL ê´€ë¦¬ ë¡œì§ ì—†ìŒ

**Production ìš”êµ¬ì‚¬í•­**:
- ì‚¬ìš©ì ì—…ë¡œë“œ Excel íŒŒì¼ì€ ì¼ì • ì‹œê°„ í›„ ìë™ ì‚­ì œ (ì˜ˆ: 7ì¼)
- DBì— TTL ì •ë³´ ì €ì¥ ë° ì£¼ê¸°ì  ì •ë¦¬

**í•„ìš”í•œ ìˆ˜ì •**:

#### A. Scratch Cleanup Service

```python
# ai_agent/services/scratch_cleanup_service.py

import asyncio
from datetime import datetime, timedelta
from pathlib import Path
import shutil
import logging

logger = logging.getLogger(__name__)


class ScratchCleanupService:
    """
    Scratch í´ë” TTL ê¸°ë°˜ ìë™ ì •ë¦¬ ì„œë¹„ìŠ¤

    ì£¼ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ë˜ì–´ ë§Œë£Œëœ Excel íŒŒì¼ ì‚­ì œ
    """

    def __init__(self, scratch_base: Path = Path("./scratch"), ttl_days: int = 7):
        self.scratch_base = scratch_base
        self.ttl_days = ttl_days
        self.logger = logger

    async def cleanup_expired_files(self, db_session):
        """
        ë§Œë£Œëœ Excel íŒŒì¼ ì •ë¦¬

        1. DBì—ì„œ expires_at < NOW() ì¸ íŒŒì¼ ì¡°íšŒ
        2. íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ì‚­ì œ
        3. DB ë ˆì½”ë“œ ì‚­ì œ ë˜ëŠ” ìƒíƒœ ì—…ë°ì´íŠ¸
        """
        cutoff_time = datetime.now() - timedelta(days=self.ttl_days)

        # 1. DBì—ì„œ ë§Œë£Œëœ íŒŒì¼ ì¡°íšŒ
        from sqlalchemy import select, delete
        from ai_agent.models import SiteExcelFile  # ê°€ìƒì˜ ëª¨ë¸

        query = select(SiteExcelFile).where(
            SiteExcelFile.expires_at < datetime.now()
        )
        result = await db_session.execute(query)
        expired_files = result.scalars().all()

        deleted_count = 0
        for file_record in expired_files:
            file_path = Path(file_record.file_path)

            # 2. íŒŒì¼ ì‚­ì œ
            if file_path.exists():
                try:
                    file_path.unlink()
                    self.logger.info(f"ì‚­ì œ ì™„ë£Œ: {file_path}")
                    deleted_count += 1
                except Exception as e:
                    self.logger.error(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ ({file_path}): {e}")

            # 3. DB ë ˆì½”ë“œ ì‚­ì œ
            await db_session.delete(file_record)

        await db_session.commit()

        self.logger.info(f"Scratch cleanup ì™„ë£Œ: {deleted_count}ê°œ íŒŒì¼ ì‚­ì œ")
        return deleted_count

    async def cleanup_empty_folders(self):
        """ë¹ˆ ì‚¬ì—…ì¥ í´ë” ì •ë¦¬"""
        if not self.scratch_base.exists():
            return

        for site_folder in self.scratch_base.iterdir():
            if site_folder.is_dir():
                # í´ë”ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì‚­ì œ
                if not any(site_folder.iterdir()):
                    shutil.rmtree(site_folder)
                    self.logger.info(f"ë¹ˆ í´ë” ì‚­ì œ: {site_folder}")


# Background task scheduler
async def schedule_cleanup(interval_hours: int = 24):
    """
    ì£¼ê¸°ì ìœ¼ë¡œ cleanup ì‹¤í–‰ (Background Task)

    FastAPI ì•± ì‹œì‘ ì‹œ lifespanì—ì„œ ì‹¤í–‰
    """
    cleanup_service = ScratchCleanupService()

    while True:
        try:
            # DB ì„¸ì…˜ ìƒì„±
            from ai_agent.db import get_async_session
            async for session in get_async_session():
                await cleanup_service.cleanup_expired_files(session)
                await cleanup_service.cleanup_empty_folders()
                break
        except Exception as e:
            logger.error(f"Cleanup ì‹¤íŒ¨: {e}")

        # 24ì‹œê°„ ëŒ€ê¸°
        await asyncio.sleep(interval_hours * 3600)
```

#### B. FastAPI Lifespan í†µí•©

```python
# main.py (FastAPI ì•±)

from contextlib import asynccontextmanager
from fastapi import FastAPI
from ai_agent.services.scratch_cleanup_service import schedule_cleanup

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ì‹œì‘/ì¢…ë£Œ ì‹œ ì‹¤í–‰ë˜ëŠ” ë¡œì§"""
    # ì•± ì‹œì‘ ì‹œ
    cleanup_task = asyncio.create_task(schedule_cleanup(interval_hours=24))
    yield
    # ì•± ì¢…ë£Œ ì‹œ
    cleanup_task.cancel()

app = FastAPI(lifespan=lifespan)
```

---

### 3. Excel File Validation & Security (Critical)

**í˜„ì¬ ìƒíƒœ**:
- Excel íŒŒì¼ ìœ íš¨ì„± ê²€ì¦ ì—†ìŒ
- ì•…ì„± íŒŒì¼ (XSS, ë§¤í¬ë¡œ, ëŒ€ìš©ëŸ‰ íŒŒì¼) ì²˜ë¦¬ ë¯¸í¡

**Production ìš”êµ¬ì‚¬í•­**:
1. **íŒŒì¼ í¬ê¸° ì œí•œ**: ìµœëŒ€ 10MB
2. **MIME íƒ€ì… ê²€ì¦**: `.xlsx`, `.xls`, `.csv` ë§Œ í—ˆìš©
3. **ë°”ì´ëŸ¬ìŠ¤ ìŠ¤ìº”**: ClamAV ë˜ëŠ” í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤
4. **ë§¤í¬ë¡œ ì œê±°**: `openpyxl`ë¡œ ì¬ì €ì¥í•˜ì—¬ ë§¤í¬ë¡œ ì œê±°
5. **í–‰/ì—´ ê°œìˆ˜ ì œí•œ**: ìµœëŒ€ 10,000í–‰ Ã— 100ì—´

**í•„ìš”í•œ ìˆ˜ì •**:

```python
# ai_agent/utils/excel_validator.py

import magic
from pathlib import Path
import pandas as pd


class ExcelValidator:
    """Excel íŒŒì¼ ìœ íš¨ì„± ê²€ì¦ ë° ë³´ì•ˆ ì²´í¬"""

    MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB
    MAX_ROWS = 10000
    MAX_COLS = 100
    ALLOWED_MIME_TYPES = [
        'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',  # .xlsx
        'application/vnd.ms-excel',  # .xls
        'text/csv'
    ]

    @staticmethod
    def validate(file_path: Path) -> dict:
        """
        Excel íŒŒì¼ ìœ íš¨ì„± ê²€ì¦

        Returns:
            {
                "valid": bool,
                "errors": List[str],
                "warnings": List[str],
                "metadata": {
                    "file_size": int,
                    "row_count": int,
                    "col_count": int
                }
            }
        """
        errors = []
        warnings = []
        metadata = {}

        # 1. íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not file_path.exists():
            return {"valid": False, "errors": ["íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"], "warnings": [], "metadata": {}}

        # 2. íŒŒì¼ í¬ê¸° ê²€ì¦
        file_size = file_path.stat().st_size
        metadata["file_size"] = file_size

        if file_size > ExcelValidator.MAX_FILE_SIZE:
            errors.append(f"íŒŒì¼ í¬ê¸° ì´ˆê³¼: {file_size / 1024 / 1024:.2f}MB > 10MB")

        # 3. MIME íƒ€ì… ê²€ì¦
        try:
            mime = magic.Magic(mime=True)
            file_mime = mime.from_file(str(file_path))

            if file_mime not in ExcelValidator.ALLOWED_MIME_TYPES:
                errors.append(f"í—ˆìš©ë˜ì§€ ì•Šì€ íŒŒì¼ íƒ€ì…: {file_mime}")
        except Exception as e:
            warnings.append(f"MIME íƒ€ì… ê²€ì¦ ì‹¤íŒ¨: {e}")

        # 4. í–‰/ì—´ ê°œìˆ˜ ê²€ì¦
        try:
            df = pd.read_excel(file_path, nrows=ExcelValidator.MAX_ROWS + 1)
            row_count = len(df)
            col_count = len(df.columns)

            metadata["row_count"] = row_count
            metadata["col_count"] = col_count

            if row_count > ExcelValidator.MAX_ROWS:
                errors.append(f"í–‰ ê°œìˆ˜ ì´ˆê³¼: {row_count} > {ExcelValidator.MAX_ROWS}")

            if col_count > ExcelValidator.MAX_COLS:
                errors.append(f"ì—´ ê°œìˆ˜ ì´ˆê³¼: {col_count} > {ExcelValidator.MAX_COLS}")
        except Exception as e:
            errors.append(f"Excel íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")

        return {
            "valid": len(errors) == 0,
            "errors": errors,
            "warnings": warnings,
            "metadata": metadata
        }
```

**ì‚¬ìš© ì˜ˆì‹œ**:

```python
from ai_agent.utils.excel_validator import ExcelValidator

# Excel íŒŒì¼ ì—…ë¡œë“œ í•¸ë“¤ëŸ¬
async def upload_excel(file: UploadFile, site_id: int):
    # 1. ì„ì‹œ ì €ì¥
    temp_path = Path(f"./scratch/{site_id}/temp_{file.filename}")
    with temp_path.open("wb") as f:
        f.write(await file.read())

    # 2. ìœ íš¨ì„± ê²€ì¦
    validation_result = ExcelValidator.validate(temp_path)

    if not validation_result["valid"]:
        temp_path.unlink()
        raise ValueError(f"ìœ íš¨í•˜ì§€ ì•Šì€ Excel íŒŒì¼: {validation_result['errors']}")

    # 3. ìµœì¢… ê²½ë¡œë¡œ ì´ë™
    final_path = Path(f"./scratch/{site_id}/additional_data.xlsx")
    temp_path.rename(final_path)

    return {
        "file_path": str(final_path),
        "metadata": validation_result["metadata"]
    }
```

---

### 4. Error Handling & Retry Logic (Critical)

**í˜„ì¬ ìƒíƒœ**:
- LLM API ì‹¤íŒ¨ ì‹œ fallback ê°€ì´ë“œë¼ì¸ë§Œ ìƒì„±
- ë„¤íŠ¸ì›Œí¬ ì¥ì• , íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ ë¯¸í¡
- ë¶€ë¶„ ì‹¤íŒ¨ ì‹œ ì „ì²´ ì‹¤íŒ¨ ì²˜ë¦¬

**Production ìš”êµ¬ì‚¬í•­**:
1. **ì¬ì‹œë„ ë¡œì§**: ì¼ì‹œì  ì˜¤ë¥˜ ì‹œ 3íšŒ ì¬ì‹œë„ (exponential backoff)
2. **ë¶€ë¶„ ì„±ê³µ ì²˜ë¦¬**: ì¼ë¶€ ì‚¬ì—…ì¥ ì‹¤íŒ¨ ì‹œ ì„±ê³µí•œ ê²°ê³¼ëŠ” ë°˜í™˜
3. **ìƒì„¸ ì—ëŸ¬ ë¡œê¹…**: Sentry ë˜ëŠ” ELK Stack ì—°ë™
4. **Circuit Breaker**: LLM API ì¥ì•  ì‹œ ìë™ fallback

**í•„ìš”í•œ ìˆ˜ì •**:

```python
# ai_agent/utils/retry_handler.py

import asyncio
from typing import Callable, Any
import logging

logger = logging.getLogger(__name__)


async def retry_with_backoff(
    func: Callable,
    max_retries: int = 3,
    base_delay: float = 1.0,
    max_delay: float = 60.0,
    exceptions: tuple = (Exception,)
) -> Any:
    """
    Exponential backoff ì¬ì‹œë„ ë¡œì§

    :param func: ë¹„ë™ê¸° í•¨ìˆ˜
    :param max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
    :param base_delay: ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
    :param max_delay: ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
    :param exceptions: ì¬ì‹œë„í•  ì˜ˆì™¸ íƒ€ì…
    :return: í•¨ìˆ˜ ì‹¤í–‰ ê²°ê³¼
    """
    for attempt in range(max_retries):
        try:
            return await func()
        except exceptions as e:
            if attempt == max_retries - 1:
                logger.error(f"ì¬ì‹œë„ ì‹¤íŒ¨ (ìµœì¢…): {e}")
                raise

            # Exponential backoff
            delay = min(base_delay * (2 ** attempt), max_delay)
            logger.warning(f"ì¬ì‹œë„ {attempt + 1}/{max_retries} (ëŒ€ê¸°: {delay:.2f}ì´ˆ): {e}")
            await asyncio.sleep(delay)


# Circuit Breaker íŒ¨í„´
class CircuitBreaker:
    """
    Circuit Breaker íŒ¨í„´ êµ¬í˜„

    ì—°ì† ì‹¤íŒ¨ ì‹œ ì¼ì • ì‹œê°„ ë™ì•ˆ ìš”ì²­ ì°¨ë‹¨ (fallback ì‚¬ìš©)
    """

    def __init__(self, failure_threshold: int = 5, timeout: int = 60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN

    async def call(self, func: Callable, fallback: Callable = None) -> Any:
        """
        Circuit Breakerë¥¼ í†µí•œ í•¨ìˆ˜ í˜¸ì¶œ

        :param func: ì‹¤í–‰í•  í•¨ìˆ˜
        :param fallback: ì‹¤íŒ¨ ì‹œ fallback í•¨ìˆ˜
        :return: í•¨ìˆ˜ ì‹¤í–‰ ê²°ê³¼
        """
        # OPEN ìƒíƒœ: timeout ê²½ê³¼ í›„ HALF_OPENìœ¼ë¡œ ì „í™˜
        if self.state == "OPEN":
            if self.last_failure_time and \
               (asyncio.get_event_loop().time() - self.last_failure_time) > self.timeout:
                self.state = "HALF_OPEN"
                logger.info("Circuit Breaker: OPEN â†’ HALF_OPEN")
            else:
                logger.warning("Circuit Breaker OPEN: fallback ì‚¬ìš©")
                if fallback:
                    return await fallback()
                raise Exception("Circuit Breaker OPEN (ì„œë¹„ìŠ¤ ì¼ì‹œ ì¤‘ë‹¨)")

        # í•¨ìˆ˜ ì‹¤í–‰ ì‹œë„
        try:
            result = await func()

            # ì„±ê³µ ì‹œ ìƒíƒœ ë¦¬ì…‹
            if self.state == "HALF_OPEN":
                self.state = "CLOSED"
                self.failure_count = 0
                logger.info("Circuit Breaker: HALF_OPEN â†’ CLOSED")

            return result

        except Exception as e:
            self.failure_count += 1
            self.last_failure_time = asyncio.get_event_loop().time()

            # ì‹¤íŒ¨ ì„ê³„ê°’ ì´ˆê³¼ ì‹œ OPEN
            if self.failure_count >= self.failure_threshold:
                self.state = "OPEN"
                logger.error(f"Circuit Breaker: CLOSED â†’ OPEN (ì—°ì† {self.failure_count}íšŒ ì‹¤íŒ¨)")

            # Fallback ì‚¬ìš©
            if fallback:
                logger.warning(f"Fallback ì‚¬ìš©: {e}")
                return await fallback()

            raise
```

**AdditionalDataAgentì— ì ìš©**:

```python
# additional_data_agent.py

from ai_agent.utils.retry_handler import retry_with_backoff, CircuitBreaker

class AdditionalDataAgent:
    def __init__(self, llm_client=None):
        self.llm_client = llm_client
        self.circuit_breaker = CircuitBreaker(failure_threshold=5, timeout=60)

    async def _generate_site_guideline(self, site_id: int, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì‚¬ì—…ì¥ë³„ ê°€ì´ë“œë¼ì¸ ìƒì„± (ì¬ì‹œë„ + Circuit Breaker)"""
        if not data:
            return self._generate_fallback_guideline(site_id, {})

        # LLM í˜¸ì¶œ í•¨ìˆ˜ ì •ì˜
        async def llm_call():
            prompt = self._build_prompt(site_id, data)

            if hasattr(self.llm_client, 'ainvoke'):
                response = await self.llm_client.ainvoke(prompt)
            else:
                response = self.llm_client.invoke(prompt)

            return {
                "site_id": site_id,
                "guideline": response,
                "key_insights": self._extract_key_insights(response)
            }

        # Fallback í•¨ìˆ˜ ì •ì˜
        async def fallback():
            self.logger.warning(f"LLM ì‹¤íŒ¨, fallback ì‚¬ìš© (ì‚¬ì—…ì¥ {site_id})")
            return self._generate_fallback_guideline(site_id, data)

        try:
            # Circuit Breaker + Retry
            result = await self.circuit_breaker.call(
                func=lambda: retry_with_backoff(
                    llm_call,
                    max_retries=3,
                    base_delay=1.0,
                    exceptions=(Exception,)
                ),
                fallback=fallback
            )
            return result

        except Exception as e:
            self.logger.error(f"LLM í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨ (ì‚¬ì—…ì¥ {site_id}): {e}")
            return await fallback()
```

---

### 5. Large-scale Processing (100+ sites) (Critical)

**í˜„ì¬ ìƒíƒœ**:
- 5ê°œ ì‚¬ì—…ì¥ í…ŒìŠ¤íŠ¸ë§Œ ì™„ë£Œ (10.47ì´ˆ)
- 100+ ì‚¬ì—…ì¥ ì‹œ ë©”ëª¨ë¦¬/ë„¤íŠ¸ì›Œí¬ ë¶€í•˜ ë¯¸ê²€ì¦

**Production ìš”êµ¬ì‚¬í•­**:
1. **ë°°ì¹˜ ì²˜ë¦¬**: 10ê°œì”© ë¬¶ì–´ì„œ ìˆœì°¨ ì‹¤í–‰ (ë©”ëª¨ë¦¬ ì œí•œ)
2. **Rate Limiting**: LLM API QPS ì œí•œ ì¤€ìˆ˜ (ì˜ˆ: 60 req/min)
3. **í”„ë¡œê·¸ë ˆìŠ¤ ì¶”ì **: ì§„í–‰ë¥  ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
4. **ë¶€ë¶„ ê²°ê³¼ ì €ì¥**: 100ê°œ ì²˜ë¦¬ ì¤‘ 50ê°œ ì™„ë£Œ ì‹œì ì— ì¤‘ê°„ ì €ì¥

**í•„ìš”í•œ ìˆ˜ì •**:

```python
# ai_agent/utils/batch_processor.py

import asyncio
from typing import List, Callable, Any
import logging

logger = logging.getLogger(__name__)


class BatchProcessor:
    """ëŒ€ê·œëª¨ ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬ê¸°"""

    def __init__(self, batch_size: int = 10, rate_limit_per_min: int = 60):
        self.batch_size = batch_size
        self.rate_limit_per_min = rate_limit_per_min
        self.requests_this_minute = 0
        self.minute_start = asyncio.get_event_loop().time()

    async def process_batches(
        self,
        items: List[Any],
        process_func: Callable,
        progress_callback: Callable = None
    ) -> List[Any]:
        """
        ì•„ì´í…œì„ ë°°ì¹˜ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬

        :param items: ì²˜ë¦¬í•  ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸
        :param process_func: ê° ì•„ì´í…œì„ ì²˜ë¦¬í•˜ëŠ” async í•¨ìˆ˜
        :param progress_callback: ì§„í–‰ë¥  ì½œë°± (processed, total)
        :return: ì²˜ë¦¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        total = len(items)
        results = []

        # ë°°ì¹˜ë¡œ ë¶„í• 
        batches = [items[i:i + self.batch_size] for i in range(0, total, self.batch_size)]

        logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(batches)}ê°œ ë°°ì¹˜, ì´ {total}ê°œ ì•„ì´í…œ")

        for batch_idx, batch in enumerate(batches):
            # Rate limiting ì²´í¬
            await self._check_rate_limit(len(batch))

            # ë°°ì¹˜ ë‚´ì—ì„œëŠ” ë³‘ë ¬ ì²˜ë¦¬
            batch_tasks = [process_func(item) for item in batch]
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

            results.extend(batch_results)

            # ì§„í–‰ë¥  ì½œë°±
            processed = (batch_idx + 1) * self.batch_size
            if progress_callback:
                await progress_callback(min(processed, total), total)

            logger.info(f"ë°°ì¹˜ {batch_idx + 1}/{len(batches)} ì™„ë£Œ ({processed}/{total})")

        return results

    async def _check_rate_limit(self, request_count: int):
        """Rate limiting ì²´í¬ (1ë¶„ë‹¹ ìš”ì²­ ìˆ˜ ì œí•œ)"""
        current_time = asyncio.get_event_loop().time()
        elapsed = current_time - self.minute_start

        # 1ë¶„ ê²½ê³¼ ì‹œ ë¦¬ì…‹
        if elapsed >= 60:
            self.requests_this_minute = 0
            self.minute_start = current_time

        # Rate limit ì´ˆê³¼ ì‹œ ëŒ€ê¸°
        if self.requests_this_minute + request_count > self.rate_limit_per_min:
            wait_time = 60 - elapsed
            logger.warning(f"Rate limit ë„ë‹¬, {wait_time:.2f}ì´ˆ ëŒ€ê¸°")
            await asyncio.sleep(wait_time)

            # ë¦¬ì…‹
            self.requests_this_minute = 0
            self.minute_start = asyncio.get_event_loop().time()

        self.requests_this_minute += request_count
```

**AdditionalDataAgentì— ì ìš©**:

```python
# additional_data_agent.py

from ai_agent.utils.batch_processor import BatchProcessor

class AdditionalDataAgent:
    async def analyze_large_scale(
        self,
        site_data: Dict[int, str],  # {site_id: excel_path}
        progress_callback: Callable = None
    ) -> Dict[str, Any]:
        """
        ëŒ€ê·œëª¨ ì‚¬ì—…ì¥ ë¶„ì„ (100+)

        :param site_data: {site_id: excel_path} ë§¤í•‘
        :param progress_callback: ì§„í–‰ë¥  ì½œë°±
        :return: ë¶„ì„ ê²°ê³¼
        """
        batch_processor = BatchProcessor(batch_size=10, rate_limit_per_min=60)

        # ê° ì‚¬ì—…ì¥ ì²˜ë¦¬ í•¨ìˆ˜
        async def process_site(site_info):
            site_id, excel_path = site_info
            return await self.analyze(excel_path, site_ids=[site_id])

        # ë°°ì¹˜ ì²˜ë¦¬
        site_items = list(site_data.items())
        results = await batch_processor.process_batches(
            site_items,
            process_site,
            progress_callback
        )

        # ê²°ê³¼ ë³‘í•©
        merged_guidelines = {}
        for (site_id, _), result in zip(site_items, results):
            if isinstance(result, Exception):
                self.logger.error(f"Site {site_id} ì‹¤íŒ¨: {result}")
                continue

            site_guidelines = result.get('site_specific_guidelines', {})
            merged_guidelines.update(site_guidelines)

        return {
            "meta": {
                "analyzed_at": datetime.now().isoformat(),
                "site_count": len(merged_guidelines),
                "total_requested": len(site_data)
            },
            "site_specific_guidelines": merged_guidelines,
            "status": "completed"
        }
```

---

## ğŸ“ Production Deployment Checklist

### Phase 1: DB & Infrastructure (Week 1-2)

- [ ] DB ìŠ¤í‚¤ë§ˆ ì„¤ê³„ ë° ë§ˆì´ê·¸ë ˆì´ì…˜
  - [ ] `additional_data_analysis_results` í…Œì´ë¸”
  - [ ] `site_excel_files` í…Œì´ë¸”
- [ ] BuildingDataFetcherì— site ID ìœ íš¨ì„± ê²€ì¦ ì¶”ê°€
- [ ] Scratch cleanup service êµ¬í˜„
  - [ ] TTL ê¸°ë°˜ íŒŒì¼ ì •ë¦¬
  - [ ] Background task scheduler
- [ ] FastAPI ë¼ì´í”„ì‚¬ì´í´ í†µí•©

### Phase 2: Security & Validation (Week 2-3)

- [ ] Excel íŒŒì¼ ìœ íš¨ì„± ê²€ì¦
  - [ ] íŒŒì¼ í¬ê¸°/MIME íƒ€ì… ì²´í¬
  - [ ] í–‰/ì—´ ê°œìˆ˜ ì œí•œ
  - [ ] ë°”ì´ëŸ¬ìŠ¤ ìŠ¤ìº” (optional)
- [ ] ì¸ì¦/ê¶Œí•œ ë¯¸ë“¤ì›¨ì–´
  - [ ] API Key ë˜ëŠ” JWT ì¸ì¦
  - [ ] ì‚¬ì—…ì¥ë³„ ì ‘ê·¼ ê¶Œí•œ ê²€ì¦
- [ ] Rate limiting (FastAPI-limiter)

### Phase 3: Robustness (Week 3-4)

- [ ] ì¬ì‹œë„ ë¡œì§ êµ¬í˜„
  - [ ] Exponential backoff
  - [ ] Circuit Breaker íŒ¨í„´
- [ ] ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”
  - [ ] ìƒì„¸ ì—ëŸ¬ ë¡œê¹… (Sentry ì—°ë™)
  - [ ] ë¶€ë¶„ ì‹¤íŒ¨ ì²˜ë¦¬
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
  - [ ] ëŒ€ìš©ëŸ‰ Excel íŒŒì¼ ìŠ¤íŠ¸ë¦¬ë° ì½ê¸°
  - [ ] ë©”ëª¨ë¦¬ ì œí•œ (cgroups)

### Phase 4: Scalability (Week 4-5)

- [ ] ë°°ì¹˜ ì²˜ë¦¬ êµ¬í˜„
  - [ ] 10ê°œì”© ë¬¶ì–´ì„œ ìˆœì°¨ ì‹¤í–‰
  - [ ] í”„ë¡œê·¸ë ˆìŠ¤ ì¶”ì  API
- [ ] LLM API Rate Limiting
  - [ ] 60 req/min ì œí•œ ì¤€ìˆ˜
  - [ ] íì‰ ì‹œìŠ¤í…œ (Celery/RQ)
- [ ] ëŒ€ê·œëª¨ í…ŒìŠ¤íŠ¸
  - [ ] 100+ ì‚¬ì—…ì¥ ë¶€í•˜ í…ŒìŠ¤íŠ¸
  - [ ] ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ (cProfile)

### Phase 5: Monitoring & Observability (Week 5-6)

- [ ] ë¡œê¹… í‘œì¤€í™”
  - [ ] Structured logging (JSON)
  - [ ] ë¡œê·¸ ë ˆë²¨ ê´€ë¦¬
- [ ] ë©”íŠ¸ë¦­ ìˆ˜ì§‘
  - [ ] Prometheus + Grafana
  - [ ] ì²˜ë¦¬ ì‹œê°„, ì„±ê³µë¥ , ì—ëŸ¬ìœ¨
- [ ] ì•Œë¦¼ ì„¤ì •
  - [ ] ì—ëŸ¬ìœ¨ ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ì•Œë¦¼
  - [ ] Slack/Email ì—°ë™

### Phase 6: Integration & E2E Testing (Week 6-7)

- [ ] ì „ì²´ Workflow í†µí•© í…ŒìŠ¤íŠ¸
  - [ ] Node 0 â†’ Node 1 â†’ Node 2-A/B/C ì—°ë™
  - [ ] State ì „ë‹¬ ê²€ì¦
- [ ] Excel ì—…ë¡œë“œ API í…ŒìŠ¤íŠ¸
  - [ ] Multipart/form-data ì²˜ë¦¬
  - [ ] íŒŒì¼ ì—…ë¡œë“œ â†’ ë¶„ì„ â†’ ê²°ê³¼ ì¡°íšŒ E2E
- [ ] ë¶€í•˜ í…ŒìŠ¤íŠ¸
  - [ ] Locust ë˜ëŠ” k6 ì‚¬ìš©
  - [ ] ë™ì‹œ 100ëª… ì‚¬ìš©ì ì‹œë®¬ë ˆì´ì…˜

---

## ğŸ”§ Code Modifications Summary

### íŒŒì¼ë³„ ìˆ˜ì •ì‚¬í•­

| íŒŒì¼ | ìˆ˜ì • ë‚´ìš© | ìš°ì„ ìˆœìœ„ |
|-----|----------|---------|
| `additional_data_agent.py` | `analyze_from_db()` ë©”ì„œë“œ ì¶”ê°€ (DB ì—°ë™) | Critical |
| `additional_data_agent.py` | `_save_results_to_db()` ë©”ì„œë“œ ì¶”ê°€ | Critical |
| `additional_data_agent.py` | ì¬ì‹œë„ + Circuit Breaker ì ìš© | Critical |
| `additional_data_agent.py` | `analyze_large_scale()` ë©”ì„œë“œ ì¶”ê°€ | Critical |
| `building_data_fetcher.py` | `validate_site_ids()` ë©”ì„œë“œ ì¶”ê°€ | Critical |
| **NEW** `services/scratch_cleanup_service.py` | TTL ê¸°ë°˜ íŒŒì¼ ì •ë¦¬ ì„œë¹„ìŠ¤ | Critical |
| **NEW** `utils/excel_validator.py` | Excel íŒŒì¼ ìœ íš¨ì„± ê²€ì¦ | Critical |
| **NEW** `utils/retry_handler.py` | ì¬ì‹œë„ + Circuit Breaker ìœ í‹¸ | Critical |
| **NEW** `utils/batch_processor.py` | ëŒ€ê·œëª¨ ë°°ì¹˜ ì²˜ë¦¬ê¸° | Critical |
| **NEW** `models/additional_data_models.py` | DB ëª¨ë¸ (SQLAlchemy) | Critical |
| **NEW** `api/excel_upload.py` | Excel ì—…ë¡œë“œ API ì—”ë“œí¬ì¸íŠ¸ | Important |
| **NEW** `api/analysis_progress.py` | ë¶„ì„ ì§„í–‰ë¥  ì¡°íšŒ API | Important |
| `main.py` (FastAPI) | Lifespan ì´ë²¤íŠ¸ ì¶”ê°€ (cleanup scheduler) | Critical |

---

## ğŸ“Š Performance Expectations

### í˜„ì¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼ (5ê°œ ì‚¬ì—…ì¥)

- **ì´ ì²˜ë¦¬ ì‹œê°„**: 10.47ì´ˆ
- **ì‚¬ì—…ì¥ë‹¹ í‰ê· **: 2.09ì´ˆ
- **ì„±ëŠ¥ í–¥ìƒ**: ìˆœì°¨ ëŒ€ë¹„ 4.8ë°° ë¹ ë¦„
- **LLM í˜¸ì¶œ**: 5ê°œ ë™ì‹œ (ë³‘ë ¬)

### Production ì˜ˆìƒ (100ê°œ ì‚¬ì—…ì¥)

**Without Optimization (ìˆœì°¨ ì²˜ë¦¬)**:
- 100 ì‚¬ì—…ì¥ Ã— 10ì´ˆ = **1,000ì´ˆ (16ë¶„ 40ì´ˆ)**

**With Parallel Processing (í˜„ì¬ êµ¬í˜„)**:
- 100 ì‚¬ì—…ì¥ ë™ì‹œ LLM í˜¸ì¶œ â†’ **ë©”ëª¨ë¦¬ í­ë°œ ìœ„í—˜** âŒ
- Rate limiting ìœ„ë°˜ (60 req/min) â†’ **API ì°¨ë‹¨** âŒ

**With Batch Processing (ê¶Œì¥)**:
- 10ê°œì”© 10ê°œ ë°°ì¹˜ â†’ ê° ë°°ì¹˜ 10ì´ˆ (ë³‘ë ¬)
- Rate limiting ì¤€ìˆ˜ â†’ 1ë¶„ë‹¹ 60ê°œ ì œí•œ
- **ì˜ˆìƒ ì‹œê°„**:
  - Batch 1-6: 1ë¶„ (60ê°œ)
  - Batch 7-10: 40ì´ˆ (40ê°œ)
  - **ì´ 100ì´ˆ (1ë¶„ 40ì´ˆ)** âœ…

**ìµœì í™” ëª©í‘œ**:
- 100ê°œ ì‚¬ì—…ì¥: **2ë¶„ ì´ë‚´**
- 500ê°œ ì‚¬ì—…ì¥: **10ë¶„ ì´ë‚´**
- 1,000ê°œ ì‚¬ì—…ì¥: **20ë¶„ ì´ë‚´**

---

## ğŸš€ Next Steps

### Immediate (This Week)

1. **DB ìŠ¤í‚¤ë§ˆ ì„¤ê³„ ë° ë§ˆì´ê·¸ë ˆì´ì…˜**
   - `additional_data_analysis_results` í…Œì´ë¸”
   - `site_excel_files` í…Œì´ë¸”

2. **Excel Validator êµ¬í˜„**
   - íŒŒì¼ í¬ê¸°/MIME íƒ€ì… ì²´í¬
   - í–‰/ì—´ ê°œìˆ˜ ì œí•œ

3. **Retry Handler êµ¬í˜„**
   - Exponential backoff
   - Circuit Breaker

### Short-term (Next 2 Weeks)

4. **Scratch Cleanup Service**
   - TTL ê¸°ë°˜ íŒŒì¼ ì •ë¦¬
   - Background task scheduler

5. **Batch Processor**
   - 10ê°œì”© ë°°ì¹˜ ì²˜ë¦¬
   - Rate limiting

6. **Integration Testing**
   - DB ì—°ë™ E2E í…ŒìŠ¤íŠ¸
   - Excel ì—…ë¡œë“œ API í…ŒìŠ¤íŠ¸

### Long-term (Next Month)

7. **Monitoring & Observability**
   - Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘
   - Grafana ëŒ€ì‹œë³´ë“œ

8. **Load Testing**
   - 100+ ì‚¬ì—…ì¥ ë¶€í•˜ í…ŒìŠ¤íŠ¸
   - ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§

9. **Production Deployment**
   - Staging í™˜ê²½ ë°°í¬
   - Production ë°°í¬

---

## ğŸ“– References

- TCFD Report v2.1 Architecture: `docs/architecture/tcfd_v2.1_overview.md`
- Primary Data Agents README: `ai_agent/agents/primary_data/README.md`
- DB Schema (DBML): `docs/database/schema.dbml`
- API Documentation: `docs/api/endpoints.md`

---

**ì‘ì„±ì**: AI Agent Team
**ê²€í† ì**: [TBD]
**ìŠ¹ì¸ì**: [TBD]
**ë‹¤ìŒ ë¦¬ë·° ì¼ì •**: 2025-12-20
