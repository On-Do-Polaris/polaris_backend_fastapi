"""
Node 4: Validator & Refiner v2.1
ìµœì¢… ìˆ˜ì •ì¼: 2025-12-17
ë²„ì „: v2.1 - LENGTH VALIDATION ADDED

ê°œìš”:
    Node 4: TCFD ê²€ì¦ ë° í’ˆì§ˆ ê´€ë¦¬ ë…¸ë“œ

    - TCFD 7ëŒ€ ì›ì¹™ ê²€ì¦: Relevant, Specific, Clear, Consistent, Comparable, Reliable, Timely
    - ë°ì´í„° ì¼ê´€ì„± ê²€ì¦
    - í’ˆì§ˆ ì ìˆ˜ ì‚°ì¶œ
    - Critical ì´ìŠˆ ë°œìƒ ì‹œ 1íšŒ ì¬ìƒì„± (ë¬´í•œ ë£¨í”„ ë°©ì§€)

ì£¼ìš” ê¸°ëŠ¥:
    1. í•„ìˆ˜ ì„¹ì…˜ ì™„ì„±ë„ ê²€ì¦
    2. ë°ì´í„° ì¼ê´€ì„± ê²€ì¦ (AAL í•©ê³„, ë¦¬ìŠ¤í¬ ìˆœìœ„ ë“±)
    3. TCFD 7ëŒ€ ì›ì¹™ ê²€ì¦
    4. í’ˆì§ˆ ì ìˆ˜ ì‚°ì¶œ (0-100ì )
    5. Critical ì´ìŠˆ ë°œìƒ ì‹œ í”¼ë“œë°± ì œê³µ

ì…ë ¥:
    - strategy: Dict (Node 3 ì¶œë ¥)
    - report_template: Dict (Node 1 ì¶œë ¥)
    - scenario_analysis: Dict (Node 2-A ì¶œë ¥)
    - impact_analyses: List[Dict] (Node 2-B ì¶œë ¥)

ì¶œë ¥:
    - validation_result: Dict (is_valid, quality_score, issues, feedback)
    - validated: bool
"""

import json
import re
from typing import Dict, Any, List, Optional


class ValidatorNode:
    """
    Node 4: Validator & Refiner ë…¸ë“œ v2

    ì—­í• :
        - TCFD ë³´ê³ ì„œ í’ˆì§ˆ ê²€ì¦
        - 7ëŒ€ ì›ì¹™ ì¤€ìˆ˜ ì—¬ë¶€ í™•ì¸
        - ë°ì´í„° ì¼ê´€ì„± ê²€ì¦
        - í’ˆì§ˆ ì ìˆ˜ ì‚°ì¶œ

    ì˜ì¡´ì„±:
        - Node 3 (Strategy Section) ì™„ë£Œ í•„ìˆ˜
        - Node 2-A, 2-B (ë°ì´í„° ì¼ê´€ì„± ê²€ì¦ìš©)
    """

    def __init__(self, llm_client=None):
        """
        Node ì´ˆê¸°í™”

        Args:
            llm_client: ainvoke ë©”ì„œë“œë¥¼ ì§€ì›í•˜ëŠ” LLM í´ë¼ì´ì–¸íŠ¸ (optional)
        """
        self.llm = llm_client

        # TCFD 7ëŒ€ ì›ì¹™
        self.tcfd_principles = [
            "Relevant",      # ê´€ë ¨ì„±: íˆ¬ììì—ê²Œ ìœ ìš©í•œ ì •ë³´
            "Specific",      # êµ¬ì²´ì„±: ì •ëŸ‰ì  ë°ì´í„° í¬í•¨
            "Clear",         # ëª…í™•ì„±: ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´
            "Consistent",    # ì¼ê´€ì„±: ë‚´ë¶€ ë°ì´í„° ì¼ì¹˜
            "Comparable",    # ë¹„êµê°€ëŠ¥ì„±: ì‹œê°„/ì¡°ì§ê°„ ë¹„êµ
            "Reliable",      # ì‹ ë¢°ì„±: ê²€ì¦ ê°€ëŠ¥í•œ ë°ì´í„°
            "Timely"         # ì ì‹œì„±: ìµœì‹  ì •ë³´ ë°˜ì˜
        ]

        # ê¸¸ì´ ê²€ì¦ ê¸°ì¤€ (v2.1 ì¶”ê°€)
        self.length_requirements = {
            "executive_summary_min_words": 150,
            "executive_summary_max_words": 300,
            "total_content_min_words": 1500,
            "total_content_max_words": 3000,
            "per_risk_min_words": 200  # ë¦¬ìŠ¤í¬ë‹¹ ìµœì†Œ ë‹¨ì–´ ìˆ˜
        }

    def _count_words(self, text: str) -> int:
        """
        í…ìŠ¤íŠ¸ì˜ ë‹¨ì–´ ìˆ˜ ê³„ì‚° (í•œê¸€ + ì˜ì–´ í˜¼í•© ì§€ì›)

        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸

        Returns:
            int: ë‹¨ì–´ ìˆ˜
        """
        if not text:
            return 0

        # í•œê¸€ ë‹¨ì–´ (ì—°ì†ëœ í•œê¸€ ë¬¸ì)
        korean_words = re.findall(r'[ê°€-í£]+', text)

        # ì˜ì–´ ë‹¨ì–´ (ì—°ì†ëœ ì˜ë¬¸ ì•ŒíŒŒë²³)
        english_words = re.findall(r'[a-zA-Z]+', text)

        # ìˆ«ì (ì—°ì†ëœ ìˆ«ìëŠ” í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ)
        numbers = re.findall(r'\d+(?:\.\d+)?%?', text)

        return len(korean_words) + len(english_words) + len(numbers)

    async def execute(
        self,
        strategy_section: Dict,
        report_template: Optional[Dict] = None,
        scenario_analysis: Optional[Dict] = None,
        impact_analyses: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """
        ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜

        Args:
            strategy_section: Node 3 ì¶œë ¥ (Strategy ì„¹ì…˜)
            report_template: Node 1 ì¶œë ¥ (optional, ë°ì´í„° ê²€ì¦ìš©)
            scenario_analysis: Node 2-A ì¶œë ¥ (optional, ë°ì´í„° ê²€ì¦ìš©)
            impact_analyses: Node 2-B ì¶œë ¥ (optional, ë°ì´í„° ê²€ì¦ìš©)

        Returns:
            Dict: ê²€ì¦ ê²°ê³¼
        """
        print("\n" + "="*80)
        print("â–¶ Node 4: Validator ê²€ì¦ ì‹œì‘")
        print("="*80)

        # 1. í•„ìˆ˜ ì„¹ì…˜ ì²´í¬ + ê¸¸ì´ ê²€ì¦ (v2.1)
        print("\n[Step 1/6] í•„ìˆ˜ ìš”ì†Œ ì™„ì„±ë„ ë° ê¸¸ì´ ê²€ì¦...")
        completeness_issues = self._check_completeness(strategy_section)
        print(f"  âœ… ì™„ì„±ë„/ê¸¸ì´ ê²€ì¦ ì™„ë£Œ ({len(completeness_issues)}ê°œ ì´ìŠˆ)")

        # 2. ë°ì´í„° ì¼ê´€ì„± ê²€ì¦
        print("\n[Step 2/6] ë°ì´í„° ì¼ê´€ì„± ê²€ì¦...")
        consistency_issues = self._check_data_consistency(
            strategy_section,
            scenario_analysis,
            impact_analyses
        )
        print(f"  âœ… ì¼ê´€ì„± ê²€ì¦ ì™„ë£Œ ({len(consistency_issues)}ê°œ ì´ìŠˆ)")

        # 3. TCFD 7ëŒ€ ì›ì¹™ ê²€ì¦
        print("\n[Step 3/6] TCFD 7ëŒ€ ì›ì¹™ ê²€ì¦...")
        principle_scores = self._check_tcfd_principles(strategy_section)
        print(f"  âœ… ì›ì¹™ ê²€ì¦ ì™„ë£Œ")

        # Rule-based ì´ìŠˆ í†µí•©
        rule_based_issues = completeness_issues + consistency_issues
        critical_count = len([i for i in rule_based_issues if i["severity"] == "critical"])

        # 4. LLM ê¸°ë°˜ ì˜ë¯¸ì  í’ˆì§ˆ ê²€ì¦ (Critical ì´ìŠˆê°€ ì—†ì„ ë•Œë§Œ ì‹¤í–‰ - ë¹„ìš© ìµœì í™”)
        print("\n[Step 4/6] LLM ê¸°ë°˜ ì˜ë¯¸ì  í’ˆì§ˆ ê²€ì¦...")
        semantic_issues = []
        if self.llm and critical_count == 0:
            print("  ğŸ” LLM semantic validation ì‹¤í–‰ ì¤‘...")
            semantic_issues = await self._check_semantic_quality(strategy_section)
            print(f"  âœ… LLM ê²€ì¦ ì™„ë£Œ ({len(semantic_issues)}ê°œ ì´ìŠˆ)")
        elif self.llm and critical_count > 0:
            print(f"  â­ï¸  Rule-based Critical ì´ìŠˆ ë°œê²¬ ({critical_count}ê±´) - LLM ê²€ì¦ ìŠ¤í‚µ (ë¹„ìš© ì ˆì•½)")
        else:
            print("  â­ï¸  LLM í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ - ìŠ¤í‚µ")

        # 5. ëª¨ë“  ì´ìŠˆ í†µí•©
        print("\n[Step 5/6] ì´ìŠˆ í†µí•© ë° í’ˆì§ˆ ì ìˆ˜ ì‚°ì¶œ...")
        all_issues = rule_based_issues + semantic_issues
        quality_score = self._calculate_quality_score(all_issues, principle_scores)
        print(f"  âœ… í’ˆì§ˆ ì ìˆ˜: {quality_score:.1f}/100")

        # 6. ì‹¤íŒ¨í•œ ë…¸ë“œ ì¶”ì¶œ (ì¬ì‹¤í–‰ ëŒ€ìƒ)
        print("\n[Step 6/6] í”¼ë“œë°± ìƒì„± ë° ì¬ì‹¤í–‰ ë…¸ë“œ ì¶”ì¶œ...")
        failed_nodes = []
        for issue in all_issues:
            if issue["severity"] == "critical" and "node" in issue:
                node = issue["node"]
                if node not in failed_nodes:
                    failed_nodes.append(node)

        # ë…¸ë“œ ìš°ì„ ìˆœìœ„ ì •ë ¬ (ìƒìœ„ ë…¸ë“œë¶€í„° ì¬ì‹¤í–‰)
        node_priority = [
            "node_2a_scenario_analysis",
            "node_2b_impact_analysis",
            "node_2c_mitigation_strategies",
            "node_3_strategy_section"
        ]
        failed_nodes = sorted(failed_nodes, key=lambda x: node_priority.index(x) if x in node_priority else 999)

        # ê²€ì¦ í†µê³¼ ì—¬ë¶€
        is_valid = len([i for i in all_issues if i["severity"] == "critical"]) == 0

        # LLM ê¸°ë°˜ í”¼ë“œë°± ìƒì„±
        if self.llm:
            feedback = await self._generate_feedback_with_llm(all_issues, strategy_section, is_valid)
        else:
            feedback = {
                "summary": self._generate_feedback(all_issues, is_valid),
                "critical_fixes": [],
                "improvement_suggestions": [],
                "retry_guidance": {}
            }

        print(f"  âœ… í”¼ë“œë°± ìƒì„± ì™„ë£Œ")

        # retry_countëŠ” stateì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ 0ìœ¼ë¡œ ì´ˆê¸°í™” (workflowì—ì„œ ê´€ë¦¬)
        retry_count = 0  # workflowì—ì„œ state.get("retry_count", 0)ë¡œ ì „ë‹¬ë°›ì•„ì•¼ í•¨

        # ê²€ì¦ ê²°ê³¼ ì¡°ë¦½
        validation_result = {
            "is_valid": is_valid,
            "quality_score": quality_score,
            "issues": all_issues,
            "principle_scores": principle_scores,
            "feedback": feedback,
            "failed_nodes": failed_nodes,  # NEW: ì¬ì‹¤í–‰ ëŒ€ìƒ ë…¸ë“œ ë¦¬ìŠ¤íŠ¸
            "retry_count": retry_count      # NEW: ì¬ì‹œë„ íšŸìˆ˜ (workflowì—ì„œ ê´€ë¦¬)
        }

        print("\n" + "="*80)
        if is_valid:
            print(f"âœ… Node 4: ê²€ì¦ í†µê³¼ (í’ˆì§ˆ ì ìˆ˜: {quality_score:.1f})")
        else:
            critical_count = len([i for i in all_issues if i["severity"] == "critical"])
            print(f"âš ï¸  Node 4: Critical ì´ìŠˆ ë°œê²¬ ({critical_count}ê±´)")
            if failed_nodes:
                print(f"ğŸ”„ ì¬ì‹¤í–‰ í•„ìš” ë…¸ë“œ: {', '.join(failed_nodes)}")
        print("="*80)

        return {
            "validation_result": validation_result,
            "validated": is_valid
        }

    def _check_completeness(self, strategy_section: Dict) -> List[Dict]:
        """
        í•„ìˆ˜ ìš”ì†Œ ì™„ì„±ë„ ê²€ì¦

        Args:
            strategy_section: Node 3 ì¶œë ¥

        Returns:
            List[Dict]: ì´ìŠˆ ë¦¬ìŠ¤íŠ¸
        """
        issues = []

        # 0. None ì²´í¬ (ë°©ì–´ì  ì½”ë”©)
        if strategy_section is None:
            issues.append({
                "severity": "critical",
                "type": "completeness",
                "field": "strategy_section",
                "message": "strategy_sectionì´ Noneì…ë‹ˆë‹¤ (Node 3 ì‹¤í–‰ ì‹¤íŒ¨ ê°€ëŠ¥ì„±)"
            })
            return issues

        # 1. í•„ìˆ˜ í•„ë“œ ì²´í¬
        required_fields = ["section_id", "title", "blocks"]
        for field in required_fields:
            if field not in strategy_section:
                issues.append({
                    "severity": "critical",
                    "type": "completeness",
                    "field": field,
                    "message": f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}"
                })

        # 2. ë¸”ë¡ ê°œìˆ˜ ì²´í¬ (ìµœì†Œ 5ê°œ ì´ìƒ í•„ìš”)
        blocks = strategy_section.get("blocks", [])
        if len(blocks) < 5:
            issues.append({
                "severity": "warning",
                "type": "completeness",
                "field": "blocks",
                "message": f"ë¸”ë¡ ê°œìˆ˜ ë¶€ì¡±: {len(blocks)}ê°œ (ìµœì†Œ 5ê°œ ê¶Œì¥)"
            })

        # 3. Executive Summary ì¡´ì¬ ì—¬ë¶€ ë° ê¸¸ì´ ê²€ì¦ (v2.1 ê°•í™”)
        has_exec_summary = False
        for block in blocks:
            if block.get("subheading") == "Executive Summary":
                has_exec_summary = True
                content = block.get("content", "")
                word_count = self._count_words(content)

                # ë‹¨ì–´ ìˆ˜ ê¸°ë°˜ ê²€ì¦ (150-300 words)
                min_words = self.length_requirements["executive_summary_min_words"]
                max_words = self.length_requirements["executive_summary_max_words"]

                if word_count < min_words:
                    issues.append({
                        "severity": "critical",
                        "type": "length",
                        "field": "executive_summary",
                        "node": "node_3_strategy_section",
                        "message": f"Executive Summaryê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ ({word_count} ë‹¨ì–´, ìµœì†Œ {min_words} ë‹¨ì–´ í•„ìš”)"
                    })
                elif word_count > max_words:
                    issues.append({
                        "severity": "warning",
                        "type": "length",
                        "field": "executive_summary",
                        "message": f"Executive Summaryê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({word_count} ë‹¨ì–´, ê¶Œì¥ {max_words} ë‹¨ì–´ ì´í•˜)"
                    })
                break

        if not has_exec_summary:
            issues.append({
                "severity": "critical",
                "type": "completeness",
                "field": "executive_summary",
                "node": "node_3_strategy_section",
                "message": "Executive Summary ëˆ„ë½"
            })

        # 4-1. ì „ì²´ ì½˜í…ì¸  ê¸¸ì´ ê²€ì¦ (v2.1 ì¶”ê°€)
        total_content = ""
        for block in blocks:
            if block.get("type") == "text":
                total_content += block.get("content", "") + " "
        total_word_count = self._count_words(total_content)

        min_total = self.length_requirements["total_content_min_words"]
        if total_word_count < min_total:
            issues.append({
                "severity": "critical",
                "type": "length",
                "field": "total_content",
                "node": "node_3_strategy_section",
                "message": f"Strategy ì„¹ì…˜ ì „ì²´ ê¸¸ì´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ ({total_word_count} ë‹¨ì–´, ìµœì†Œ {min_total} ë‹¨ì–´ í•„ìš”)"
            })

        # 4. HeatmapTableBlock ì¡´ì¬ ì—¬ë¶€
        has_heatmap = False
        for block in blocks:
            if block.get("type") == "heatmap_table":
                has_heatmap = True
                break

        if not has_heatmap:
            issues.append({
                "severity": "warning",
                "type": "completeness",
                "field": "heatmap_table",
                "message": "HeatmapTableBlock ëˆ„ë½ (ê¶Œì¥)"
            })

        return issues

    def _check_data_consistency(
        self,
        strategy_section: Dict,
        scenario_analysis: Optional[Dict],
        impact_analyses: Optional[List[Dict]]
    ) -> List[Dict]:
        """
        ë°ì´í„° ì¼ê´€ì„± ê²€ì¦

        Args:
            strategy_section: Node 3 ì¶œë ¥
            scenario_analysis: Node 2-A ì¶œë ¥ (optional)
            impact_analyses: Node 2-B ì¶œë ¥ (optional)

        Returns:
            List[Dict]: ì´ìŠˆ ë¦¬ìŠ¤íŠ¸
        """
        issues = []

        # 1. HeatmapTableê³¼ Impact Analyses ì¼ê´€ì„± ì²´í¬
        if impact_analyses:
            heatmap_block = None
            for block in strategy_section.get("blocks", []):
                if block.get("type") == "heatmap_table":
                    heatmap_block = block
                    break

            if heatmap_block:
                # Heatmap í—¤ë”ì˜ ë¦¬ìŠ¤í¬ ê°œìˆ˜ vs Impact Analyses ê°œìˆ˜
                headers = heatmap_block.get("data", {}).get("headers", [])
                # ì²« ë²ˆì§¸ëŠ” "ì‚¬ì—…ì¥", ë§ˆì§€ë§‰ì€ "Total AAL"ì´ë¯€ë¡œ ì œì™¸
                risk_count_in_heatmap = len(headers) - 2
                impact_count = len(impact_analyses)

                if risk_count_in_heatmap != impact_count:
                    issues.append({
                        "severity": "warning",
                        "type": "consistency",
                        "field": "heatmap_risks",
                        "message": f"Heatmap ë¦¬ìŠ¤í¬ ê°œìˆ˜({risk_count_in_heatmap})ì™€ Impact Analysis ê°œìˆ˜({impact_count})ê°€ ë¶ˆì¼ì¹˜"
                    })

        # 2. Priority Actions Tableê³¼ Impact Analyses ì¼ê´€ì„± ì²´í¬
        priority_table = strategy_section.get("priority_actions_table")
        if priority_table and impact_analyses:
            rows = priority_table.get("data", {}).get("rows", [])
            if len(rows) != len(impact_analyses):
                issues.append({
                    "severity": "warning",
                    "type": "consistency",
                    "field": "priority_table",
                    "message": f"Priority Table í–‰ ê°œìˆ˜({len(rows)})ì™€ Impact Analysis ê°œìˆ˜({len(impact_analyses)})ê°€ ë¶ˆì¼ì¹˜"
                })

        # 3. AAL ê°’ ë²”ìœ„ ì²´í¬
        # ì°¸ê³ : total_aalì€ ë©€í‹° ì‚¬ì´íŠ¸ í•©ê³„ì´ë¯€ë¡œ 100%ë¥¼ ì´ˆê³¼í•  ìˆ˜ ìˆìŒ (ì •ìƒ)
        # ê°œë³„ ì‚¬ì´íŠ¸ AALë§Œ 0-100% ë²”ìœ„ ê²€ì¦
        if impact_analyses:
            for i, impact in enumerate(impact_analyses, 1):
                # ê°œë³„ ì‚¬ì´íŠ¸ë³„ AAL ê²€ì¦ (site_aal_valuesê°€ ìˆëŠ” ê²½ìš°)
                site_aal_values = impact.get("site_aal_values", {})
                for site_name, aal in site_aal_values.items():
                    if aal < 0 or aal > 100:
                        issues.append({
                            "severity": "critical",
                            "type": "data_validity",
                            "field": f"impact_aal_p{i}_{site_name}",
                            "message": f"P{i} {site_name} AAL ê°’ì´ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨: {aal}% (0-100% ë²”ìœ„)"
                        })

                # total_aalì´ ìŒìˆ˜ì¸ ê²½ìš°ë§Œ ì—ëŸ¬ (í•©ê³„ > 100%ëŠ” ë©€í‹°ì‚¬ì´íŠ¸ì—ì„œ ì •ìƒ)
                total_aal = impact.get("total_aal", 0.0)
                if total_aal < 0:
                    issues.append({
                        "severity": "critical",
                        "type": "data_validity",
                        "field": f"impact_aal_p{i}",
                        "message": f"P{i} AAL ê°’ì´ ìŒìˆ˜: {total_aal}%"
                    })

        return issues

    async def _check_semantic_quality(self, strategy_section: Dict) -> List[Dict]:
        """
        LLM ê¸°ë°˜ ì˜ë¯¸ì  í’ˆì§ˆ ê²€ì¦

        Args:
            strategy_section: Node 3 ì¶œë ¥

        Returns:
            List[Dict]: ì˜ë¯¸ì  ì´ìŠˆ ë¦¬ìŠ¤íŠ¸
        """
        if not self.llm:
            return []  # LLMì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ

        # strategy_sectionì„ JSON stringìœ¼ë¡œ ë³€í™˜ (pretty print)
        section_json = json.dumps(strategy_section, indent=2, ensure_ascii=False)

        prompt = f"""You are a TCFD report quality validator. Check semantic quality of the report section.

<REPORT_SECTION>
{section_json}
</REPORT_SECTION>

<VALIDATION_CRITERIA>
1. **Logical Consistency**: Do risk levels match AAL values?
   - AAL 0-3%: Low risk
   - AAL 3-10%: Medium risk
   - AAL 10-30%: High risk
   - AAL 30%+: Very high risk

2. **TCFD Principles**:
   - Relevant: Is this useful for investors?
   - Specific: Are quantitative data meaningful?
   - Clear: Is language unambiguous?
   - Consistent: Do all sections align?

3. **Completeness**:
   - Does Executive Summary cover key findings?
   - Are mitigation strategies actionable?
   - Are timelines realistic?

4. **Data Interpretation**:
   - Are AAL values interpreted correctly?
   - Do risk rankings make sense?
   - Are mitigation costs reasonable?
</VALIDATION_CRITERIA>

<OUTPUT_FORMAT>
Generate JSON array of semantic issues found:

[
  {{
    "severity": "critical" | "warning",
    "type": "semantic",
    "category": "logical_consistency" | "tcfd_principle" | "completeness" | "data_interpretation",
    "message": "Brief description",
    "evidence": "Quote from report showing the issue",
    "suggestion": "How to fix",
    "node": "node_2b_impact_analysis" | "node_2c_mitigation_strategies" | "node_3_strategy_section"
  }}
]

**IMPORTANT**: The "node" field must indicate which node needs to be re-executed to fix the issue:
- "node_2a_scenario_analysis" - for scenario analysis issues
- "node_2b_impact_analysis" - for risk ranking, AAL interpretation issues
- "node_2c_mitigation_strategies" - for mitigation strategy issues
- "node_3_strategy_section" - for overall structure, Executive Summary issues

If no semantic issues found, return empty array: []

Output pure JSON only. No markdown, no code blocks.
</OUTPUT_FORMAT>"""

        try:
            result = await self.llm.ainvoke(prompt)

            # LLM ì‘ë‹µì´ ë¬¸ìì—´ì¸ ê²½ìš° (ëŒ€ë¶€ë¶„ì˜ LLM)
            if isinstance(result, str):
                content = result
            # LangChain AIMessage ê°ì²´ì¸ ê²½ìš°
            elif hasattr(result, 'content'):
                content = result.content
            else:
                content = str(result)

            # Markdown code block ì œê±° (```json ... ```)
            content = content.strip()
            if content.startswith("```json"):
                content = content[7:]
            elif content.startswith("```"):
                content = content[3:]
            if content.endswith("```"):
                content = content[:-3]
            content = content.strip()

            # JSON íŒŒì‹±
            issues = json.loads(content)

            # ê²°ê³¼ ê²€ì¦
            if not isinstance(issues, list):
                print(f"  âš ï¸  LLM semantic validation returned non-list: {type(issues)}")
                return []

            return issues

        except json.JSONDecodeError as e:
            print(f"  âš ï¸  LLM semantic validation JSON parse error: {e}")
            print(f"  Raw response: {content[:200]}...")
            return []
        except Exception as e:
            print(f"  âš ï¸  LLM semantic validation error: {e}")
            return []

    def _check_tcfd_principles(self, strategy_section: Dict) -> Dict[str, float]:
        """
        TCFD 7ëŒ€ ì›ì¹™ ê²€ì¦

        Args:
            strategy_section: Node 3 ì¶œë ¥

        Returns:
            Dict[str, float]: ì›ì¹™ë³„ ì ìˆ˜ (0-100)
        """
        scores = {}

        # 1. Relevant (ê´€ë ¨ì„±): Executive Summary ì¡´ì¬ ì—¬ë¶€
        has_exec_summary = any(
            b.get("subheading") == "Executive Summary"
            for b in strategy_section.get("blocks", [])
        )
        scores["Relevant"] = 100.0 if has_exec_summary else 50.0

        # 2. Specific (êµ¬ì²´ì„±): ì •ëŸ‰ì  ë°ì´í„° í¬í•¨ ì—¬ë¶€ (HeatmapTable, Priority Table)
        has_heatmap = any(
            b.get("type") == "heatmap_table"
            for b in strategy_section.get("blocks", [])
        )
        has_priority_table = strategy_section.get("priority_actions_table") is not None
        scores["Specific"] = 100.0 if (has_heatmap and has_priority_table) else 70.0

        # 3. Clear (ëª…í™•ì„±): ë¸”ë¡ êµ¬ì¡°í™” ì—¬ë¶€
        blocks = strategy_section.get("blocks", [])
        has_structure = len(blocks) >= 5 and all(
            "type" in b for b in blocks
        )
        scores["Clear"] = 100.0 if has_structure else 70.0

        # 4. Consistent (ì¼ê´€ì„±): ë°ì´í„° ê²€ì¦ì€ _check_data_consistencyì—ì„œ ìˆ˜í–‰
        scores["Consistent"] = 90.0  # ê¸°ë³¸ ì ìˆ˜ (ë°ì´í„° ì¼ê´€ì„± ì´ìŠˆê°€ ìˆìœ¼ë©´ ê°ì )

        # 5. Comparable (ë¹„êµê°€ëŠ¥ì„±): ì‹œë‚˜ë¦¬ì˜¤/ë¦¬ìŠ¤í¬ ë¹„êµ ê°€ëŠ¥ ì—¬ë¶€
        scores["Comparable"] = 85.0  # ê¸°ë³¸ ì ìˆ˜

        # 6. Reliable (ì‹ ë¢°ì„±): ë°ì´í„° ì¶œì²˜ ëª…í™•ì„±
        scores["Reliable"] = 90.0  # ê¸°ë³¸ ì ìˆ˜

        # 7. Timely (ì ì‹œì„±): ìµœì‹  ë°ì´í„° ì‚¬ìš© ì—¬ë¶€
        scores["Timely"] = 95.0  # ê¸°ë³¸ ì ìˆ˜ (2025ë…„ ê¸°ì¤€)

        return scores

    def _calculate_quality_score(
        self,
        issues: List[Dict],
        principle_scores: Dict[str, float]
    ) -> float:
        """
        í’ˆì§ˆ ì ìˆ˜ ì‚°ì¶œ (0-100ì )

        Args:
            issues: ì´ìŠˆ ë¦¬ìŠ¤íŠ¸
            principle_scores: TCFD ì›ì¹™ë³„ ì ìˆ˜

        Returns:
            float: í’ˆì§ˆ ì ìˆ˜
        """
        # 1. ê¸°ë³¸ ì ìˆ˜ (TCFD ì›ì¹™ í‰ê· )
        base_score = sum(principle_scores.values()) / len(principle_scores)

        # 2. ì´ìŠˆ ê°ì 
        deduction = 0.0
        for issue in issues:
            if issue["severity"] == "critical":
                deduction += 20.0  # Critical ì´ìŠˆë‹¹ -20ì 
            elif issue["severity"] == "warning":
                deduction += 5.0   # Warning ì´ìŠˆë‹¹ -5ì 

        # 3. ìµœì¢… ì ìˆ˜ (ìµœì†Œ 0ì , ìµœëŒ€ 100ì )
        final_score = max(0.0, min(100.0, base_score - deduction))

        return final_score

    async def _generate_feedback_with_llm(
        self,
        issues: List[Dict],
        strategy_section: Dict,
        is_valid: bool
    ) -> Dict[str, Any]:
        """
        LLM ê¸°ë°˜ í”¼ë“œë°± ìƒì„± (êµ¬ì¡°í™”ëœ í”¼ë“œë°±)

        Args:
            issues: ì´ìŠˆ ë¦¬ìŠ¤íŠ¸ (Rule + LLM ê²°í•©)
            strategy_section: Node 3 ì¶œë ¥
            is_valid: ê²€ì¦ í†µê³¼ ì—¬ë¶€

        Returns:
            Dict: êµ¬ì¡°í™”ëœ í”¼ë“œë°±
        """
        if not self.llm:
            # LLMì´ ì—†ìœ¼ë©´ ê¸°ì¡´ Rule-based í”¼ë“œë°± ì‚¬ìš©
            return {
                "summary": self._generate_feedback(issues, is_valid),
                "critical_fixes": [],
                "improvement_suggestions": [],
                "retry_guidance": {}
            }

        if is_valid:
            return {
                "summary": "âœ… All validation checks passed.",
                "critical_fixes": [],
                "improvement_suggestions": [],
                "retry_guidance": {}
            }

        # Critical/Warning ì´ìŠˆ ë¶„ë¥˜
        critical_issues = [i for i in issues if i["severity"] == "critical"]
        warning_issues = [i for i in issues if i["severity"] == "warning"]

        # strategy_sectionì˜ ì¼ë¶€ë§Œ ì „ë‹¬ (í† í° ì ˆì•½)
        section_excerpt = json.dumps(strategy_section, indent=2, ensure_ascii=False)[:2000]

        prompt = f"""You are a TCFD report validator. Generate specific, actionable feedback for validation issues.

<VALIDATION_ISSUES>
Critical Issues ({len(critical_issues)}):
{json.dumps(critical_issues, indent=2, ensure_ascii=False)}

Warnings ({len(warning_issues)}):
{json.dumps(warning_issues, indent=2, ensure_ascii=False)}
</VALIDATION_ISSUES>

<REPORT_EXCERPT>
{section_excerpt}
... (truncated)
</REPORT_EXCERPT>

<OUTPUT_FORMAT>
Generate structured feedback in JSON:

{{
  "summary": "Brief overview of validation status (2-3 sentences)",
  "critical_fixes": [
    {{
      "issue": "Issue description",
      "node": "node_2b_impact_analysis" | "node_2c_mitigation_strategies" | "node_3_strategy_section",
      "action": "Specific steps to fix (2-3 sentences)",
      "example": "Expected output example (1-2 sentences)"
    }}
  ],
  "improvement_suggestions": [
    "Suggestion 1 (for warning-level issues)",
    "Suggestion 2"
  ],
  "retry_guidance": {{
    "node_2a_scenario_analysis": "Guidance for this node (if needed)",
    "node_2b_impact_analysis": "Ensure risk ranking matches AAL values",
    "node_2c_mitigation_strategies": "Add financial estimates and timelines",
    "node_3_strategy_section": "Improve Executive Summary coverage"
  }}
}}

**IMPORTANT**:
- "critical_fixes": Only include issues with severity="critical"
- "improvement_suggestions": Include issues with severity="warning"
- "retry_guidance": Provide specific guidance for nodes that need re-execution
- Keep all text concise and actionable

Output pure JSON only. No markdown, no code blocks.
</OUTPUT_FORMAT>"""

        try:
            result = await self.llm.ainvoke(prompt)

            # LLM ì‘ë‹µ íŒŒì‹± (ë™ì¼í•œ ë¡œì§)
            if isinstance(result, str):
                content = result
            elif hasattr(result, 'content'):
                content = result.content
            else:
                content = str(result)

            # Markdown code block ì œê±°
            content = content.strip()
            if content.startswith("```json"):
                content = content[7:]
            elif content.startswith("```"):
                content = content[3:]
            if content.endswith("```"):
                content = content[:-3]
            content = content.strip()

            # JSON íŒŒì‹±
            feedback = json.loads(content)

            # ê²°ê³¼ ê²€ì¦
            if not isinstance(feedback, dict):
                print(f"  âš ï¸  LLM feedback returned non-dict: {type(feedback)}")
                return {
                    "summary": self._generate_feedback(issues, is_valid),
                    "critical_fixes": [],
                    "improvement_suggestions": [],
                    "retry_guidance": {}
                }

            return feedback

        except json.JSONDecodeError as e:
            print(f"  âš ï¸  LLM feedback JSON parse error: {e}")
            print(f"  Raw response: {content[:200]}...")
            # Fallback to rule-based feedback
            return {
                "summary": self._generate_feedback(issues, is_valid),
                "critical_fixes": [],
                "improvement_suggestions": [],
                "retry_guidance": {}
            }
        except Exception as e:
            print(f"  âš ï¸  LLM feedback generation error: {e}")
            # Fallback to rule-based feedback
            return {
                "summary": self._generate_feedback(issues, is_valid),
                "critical_fixes": [],
                "improvement_suggestions": [],
                "retry_guidance": {}
            }

    def _generate_feedback(self, issues: List[Dict], is_valid: bool) -> str:
        """
        í”¼ë“œë°± ë©”ì‹œì§€ ìƒì„±

        Args:
            issues: ì´ìŠˆ ë¦¬ìŠ¤íŠ¸
            is_valid: ê²€ì¦ í†µê³¼ ì—¬ë¶€

        Returns:
            str: í”¼ë“œë°± ë©”ì‹œì§€
        """
        if is_valid and len(issues) == 0:
            return "âœ… ëª¨ë“  ê²€ì¦ í•­ëª©ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤. TCFD ë³´ê³ ì„œ í’ˆì§ˆì´ ìš°ìˆ˜í•©ë‹ˆë‹¤."

        feedback_parts = []

        if not is_valid:
            critical_issues = [i for i in issues if i["severity"] == "critical"]
            feedback_parts.append(f"âš ï¸  {len(critical_issues)}ê°œì˜ Critical ì´ìŠˆê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤:")
            for issue in critical_issues[:3]:  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                feedback_parts.append(f"  - {issue['message']}")

        warning_issues = [i for i in issues if i["severity"] == "warning"]
        if warning_issues:
            feedback_parts.append(f"\nğŸ“‹ {len(warning_issues)}ê°œì˜ Warningì´ ìˆìŠµë‹ˆë‹¤:")
            for issue in warning_issues[:3]:  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                feedback_parts.append(f"  - {issue['message']}")

        if not feedback_parts:
            return "ê²€ì¦ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."

        return "\n".join(feedback_parts)
