"""
íŒŒì¼ëª…: rag_ingestion_service.py
ìµœì¢… ìˆ˜ì •ì¼: 2025-12-12
ë²„ì „: v01
íŒŒì¼ ê°œìš”: LlamaParse íŒŒì‹± ê²°ê³¼ë¥¼ Qdrantì— ì €ì¥í•˜ëŠ” RAG Ingestion ì„œë¹„ìŠ¤

ì£¼ìš” ê¸°ëŠ¥:
- DocumentParserë¥¼ ì‚¬ìš©í•˜ì—¬ PDF íŒŒì‹±
- íŒŒì‹±ëœ ë¬¸ì„œë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ì²­í‚¹
- ë‘ ê°œì˜ Qdrant ì»¬ë ‰ì…˜ì— ì €ì¥:
  1. tcfd_documents: ì¼ë°˜ ë¬¸ì„œ ì²­í¬
  2. tcfd_tables: í…Œì´ë¸” ë°ì´í„°
- ë©”íƒ€ë°ì´í„° ê´€ë¦¬ ë° ì¶”ì 

ì£¼ì˜ì‚¬í•­:
- ì‹¤ì œ ì‹¤í–‰ì€ scripts/ingest_rag_documents.pyì—ì„œë§Œ ìˆ˜í–‰
- ë¡œì»¬ í™˜ê²½ì—ì„œ Qdrant ì—°ê²° ë¶ˆê°€ ì‹œ ì½”ë“œ êµ¬ì¡°ë§Œ í™•ì¸
"""

import os
import json
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path
from datetime import datetime
import re

from ai_agent.services.document_parser import DocumentParser
from ai_agent.utils.qdrant_vector_store import QdrantVectorStore

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)


class RAGIngestionService:
    """
    RAG ë¬¸ì„œ ìˆ˜ì§‘ ë° Qdrant ì €ì¥ ì„œë¹„ìŠ¤

    ì›Œí¬í”Œë¡œìš°:
    1. DocumentParserë¡œ PDF íŒŒì‹± (ìºì‹œ ìš°ì„ )
    2. í…ìŠ¤íŠ¸ ì²­í‚¹ (512 í† í° ë‹¨ìœ„)
    3. í…Œì´ë¸” ì¶”ì¶œ ë° ë¶„ë¦¬ ì €ì¥
    4. Qdrantì— ì—…ë¡œë“œ (2ê°œ ì»¬ë ‰ì…˜)
    """

    def __init__(
        self,
        qdrant_url: str = "http://localhost:6333",
        qdrant_api_key: Optional[str] = None,
        chunk_size: int = 512,
        chunk_overlap: int = 50
    ):
        """
        Args:
            qdrant_url: Qdrant ì„œë²„ URL
            qdrant_api_key: Qdrant API í‚¤
            chunk_size: ì²­í‚¹ í¬ê¸° (í† í° ë‹¨ìœ„)
            chunk_overlap: ì²­í¬ ê°„ ì˜¤ë²„ë© í¬ê¸°
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

        # DocumentParser ì´ˆê¸°í™”
        self.parser = DocumentParser()

        # Qdrant Vector Stores ì´ˆê¸°í™” (2ê°œ ì»¬ë ‰ì…˜)
        try:
            # 1. ì¼ë°˜ ë¬¸ì„œ ì»¬ë ‰ì…˜
            self.doc_store = QdrantVectorStore(
                url=qdrant_url,
                api_key=qdrant_api_key,
                collection_name="tcfd_documents"
            )

            # 2. í…Œì´ë¸” ì „ìš© ì»¬ë ‰ì…˜
            self.table_store = QdrantVectorStore(
                url=qdrant_url,
                api_key=qdrant_api_key,
                collection_name="tcfd_tables"
            )

            logger.info("Qdrant stores initialized successfully")

        except Exception as e:
            logger.warning(f"Failed to initialize Qdrant: {e}")
            logger.warning("Running in offline mode (structure validation only)")
            self.doc_store = None
            self.table_store = None

    def ingest_pdf(
        self,
        file_path: str,
        document_metadata: Dict[str, Any],
        force_reparse: bool = False
    ) -> Dict[str, Any]:
        """
        PDF íŒŒì‹± ë° Qdrant ì €ì¥

        Args:
            file_path: PDF íŒŒì¼ ê²½ë¡œ
            document_metadata: ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
                {
                    'document_id': 'tcfd_official_2017',
                    'document_type': 'guideline',  # guideline, benchmark, analysis
                    'source': 'TCFD',
                    'year': 2017,
                    'title': 'Final Report: Recommendations of the TCFD',
                    'tags': ['tcfd', 'governance', 'strategy']
                }
            force_reparse: ìºì‹œ ë¬´ì‹œí•˜ê³  ì¬íŒŒì‹± ì—¬ë¶€

        Returns:
            ìˆ˜ì§‘ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logger.info(f"Starting ingestion for: {file_path}")

        # 1. PDF íŒŒì‹±
        parsed_docs = self.parser.parse_pdf(file_path, force_reparse)
        logger.info(f"Parsed {len(parsed_docs)} document chunks")

        # 2. ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•©
        full_text = "\n\n".join([doc['text'] for doc in parsed_docs])

        # 3. í…ìŠ¤íŠ¸ ì²­í‚¹
        text_chunks = self._chunk_text(full_text)
        logger.info(f"Created {len(text_chunks)} text chunks")

        # 4. í…Œì´ë¸” ì¶”ì¶œ
        tables = self.parser.extract_tables_from_markdown(full_text)
        logger.info(f"Extracted {len(tables)} tables")

        # 5. Qdrantì— ì €ì¥
        result = {
            'document_id': document_metadata.get('document_id'),
            'file_path': file_path,
            'parsed_chunks': len(parsed_docs),
            'text_chunks_created': len(text_chunks),
            'tables_extracted': len(tables),
            'uploaded_to_qdrant': False
        }

        if self.doc_store and self.table_store:
            # ì¼ë°˜ ë¬¸ì„œ ì²­í¬ ì—…ë¡œë“œ
            doc_count = self._upload_text_chunks(
                text_chunks,
                document_metadata
            )

            # í…Œì´ë¸” ì—…ë¡œë“œ
            table_count = self._upload_tables(
                tables,
                document_metadata
            )

            result['uploaded_to_qdrant'] = True
            result['documents_uploaded'] = doc_count
            result['tables_uploaded'] = table_count

            logger.info(f"âœ… Ingestion completed: {doc_count} docs, {table_count} tables")
        else:
            logger.warning("âš ï¸  Qdrant not available - skipping upload (structure validated)")

        return result

    def _chunk_text(
        self,
        text: str,
        max_chunk_size: int = None
    ) -> List[Dict[str, Any]]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì²­í‚¹

        ì „ëµ:
        1. ì„¹ì…˜/í—¤ë” ë‹¨ìœ„ë¡œ 1ì°¨ ë¶„í• 
        2. í¬ê¸° ì´ˆê³¼ ì‹œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ 2ì°¨ ë¶„í• 
        3. ì˜¤ë²„ë© ì ìš©í•˜ì—¬ ë¬¸ë§¥ ë³´ì¡´

        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            max_chunk_size: ìµœëŒ€ ì²­í¬ í¬ê¸° (ë¬¸ì ë‹¨ìœ„, ê¸°ë³¸ê°’: self.chunk_size * 4)

        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸ [{'text': ..., 'chunk_index': ...}, ...]
        """
        if max_chunk_size is None:
            # ëŒ€ëµ 1 í† í° = 4 ê¸€ìë¡œ ì¶”ì •
            max_chunk_size = self.chunk_size * 4

        chunks = []

        # Markdown í—¤ë”ë¡œ 1ì°¨ ë¶„í•  (ì„¹ì…˜ ë‹¨ìœ„)
        sections = re.split(r'\n(#{1,3}\s+.+)\n', text)

        current_chunk = ""
        chunk_index = 0

        for section in sections:
            # ë¹ˆ ì„¹ì…˜ ìŠ¤í‚µ
            if not section.strip():
                continue

            # í˜„ì¬ ì²­í¬ì— ì¶”ê°€ ì‹œ í¬ê¸° í™•ì¸
            if len(current_chunk) + len(section) < max_chunk_size:
                current_chunk += section + "\n"
            else:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                if current_chunk.strip():
                    chunks.append({
                        'text': current_chunk.strip(),
                        'chunk_index': chunk_index,
                        'char_count': len(current_chunk)
                    })
                    chunk_index += 1

                # ìƒˆ ì²­í¬ ì‹œì‘ (ì˜¤ë²„ë© ì ìš©)
                overlap_text = self._get_overlap_text(current_chunk)
                current_chunk = overlap_text + section + "\n"

        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk.strip():
            chunks.append({
                'text': current_chunk.strip(),
                'chunk_index': chunk_index,
                'char_count': len(current_chunk)
            })

        return chunks

    def _get_overlap_text(self, text: str) -> str:
        """
        ì²­í¬ ì˜¤ë²„ë©ìš© í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë§ˆì§€ë§‰ N ë¬¸ì)

        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸

        Returns:
            ì˜¤ë²„ë© í…ìŠ¤íŠ¸
        """
        overlap_size = self.chunk_overlap * 4  # í† í° â†’ ë¬¸ì ë³€í™˜

        if len(text) < overlap_size:
            return text

        # ë§ˆì§€ë§‰ N ë¬¸ìì—ì„œ ë¬¸ì¥ ê²½ê³„ ì°¾ê¸°
        overlap_text = text[-overlap_size:]

        # ë¬¸ì¥ ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸° (ë§ˆì¹¨í‘œ ë’¤)
        sentence_start = overlap_text.find('. ')
        if sentence_start != -1:
            return overlap_text[sentence_start + 2:]

        return overlap_text

    def _upload_text_chunks(
        self,
        chunks: List[Dict[str, Any]],
        metadata: Dict[str, Any]
    ) -> int:
        """
        í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ tcfd_documents ì»¬ë ‰ì…˜ì— ì—…ë¡œë“œ

        Args:
            chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
            metadata: ë¬¸ì„œ ë©”íƒ€ë°ì´í„°

        Returns:
            ì—…ë¡œë“œëœ ì²­í¬ ê°œìˆ˜
        """
        documents = []

        for chunk in chunks:
            doc = {
                'id': f"{metadata['document_id']}_chunk_{chunk['chunk_index']}",
                'content': chunk['text'],
                'company_name': metadata.get('source', 'Unknown'),
                'report_year': metadata.get('year', 2024),
                'report_type': metadata.get('document_type', 'guideline'),
                'section_type': 'general',  # ì„¹ì…˜ ìë™ ê°ì§€ ì‹œ ì—…ë°ì´íŠ¸
                'content_summary': chunk['text'][:200] + '...' if len(chunk['text']) > 200 else chunk['text'],
                'metadata': {
                    'document_id': metadata['document_id'],
                    'title': metadata.get('title', 'Unknown'),
                    'chunk_index': chunk['chunk_index'],
                    'char_count': chunk['char_count'],
                    'tags': metadata.get('tags', [])
                },
                'tags': metadata.get('tags', [])
            }
            documents.append(doc)

        # Qdrant ì—…ë¡œë“œ
        uploaded_count = self.doc_store.add_documents(documents)
        logger.info(f"Uploaded {uploaded_count} text chunks to tcfd_documents")

        return uploaded_count

    def _upload_tables(
        self,
        tables: List[Dict[str, Any]],
        metadata: Dict[str, Any]
    ) -> int:
        """
        í…Œì´ë¸”ì„ tcfd_tables ì»¬ë ‰ì…˜ì— ì—…ë¡œë“œ

        Args:
            tables: í…Œì´ë¸” ë¦¬ìŠ¤íŠ¸
            metadata: ë¬¸ì„œ ë©”íƒ€ë°ì´í„°

        Returns:
            ì—…ë¡œë“œëœ í…Œì´ë¸” ê°œìˆ˜
        """
        documents = []

        for idx, table in enumerate(tables):
            # í…Œì´ë¸” ë‚´ìš©ì„ ìì—°ì–´ë¡œ ë³€í™˜ (ê²€ìƒ‰ ìµœì í™”)
            table_text = self._table_to_text(table['data'])

            doc = {
                'id': f"{metadata['document_id']}_table_{idx}",
                'content': table_text,
                'company_name': metadata.get('source', 'Unknown'),
                'report_year': metadata.get('year', 2024),
                'report_type': metadata.get('document_type', 'guideline'),
                'section_type': 'table',
                'content_summary': f"Table with {len(table['data']['headers'])} columns and {len(table['data']['rows'])} rows",
                'metadata': {
                    'document_id': metadata['document_id'],
                    'title': metadata.get('title', 'Unknown'),
                    'table_index': idx,
                    'markdown': table['markdown'],
                    'headers': table['data']['headers'],
                    'row_count': len(table['data']['rows']),
                    'column_count': len(table['data']['headers']),
                    'tags': metadata.get('tags', [])
                },
                'tags': metadata.get('tags', []) + ['table']
            }
            documents.append(doc)

        if documents:
            uploaded_count = self.table_store.add_documents(documents)
            logger.info(f"Uploaded {uploaded_count} tables to tcfd_tables")
            return uploaded_count

        return 0

    def _table_to_text(self, table_data: Dict[str, Any]) -> str:
        """
        í…Œì´ë¸” ë°ì´í„°ë¥¼ ìì—°ì–´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì„ë² ë”© ìµœì í™”)

        Args:
            table_data: {'headers': [...], 'rows': [[...], ...]}

        Returns:
            ìì—°ì–´ í…ìŠ¤íŠ¸
        """
        headers = table_data['headers']
        rows = table_data['rows']

        # í—¤ë” ì„¤ëª…
        text_parts = [
            f"This table contains {len(rows)} rows with the following columns: {', '.join(headers)}."
        ]

        # ê° í–‰ì„ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜
        for row in rows[:5]:  # ìµœëŒ€ 5ê°œ í–‰ë§Œ í¬í•¨ (ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šë„ë¡)
            row_text = " | ".join([f"{headers[i]}: {cell}" for i, cell in enumerate(row) if i < len(headers)])
            text_parts.append(row_text)

        if len(rows) > 5:
            text_parts.append(f"... and {len(rows) - 5} more rows.")

        return "\n".join(text_parts)

    def get_ingestion_stats(self) -> Dict[str, Any]:
        """
        RAG ìˆ˜ì§‘ í†µê³„ ë°˜í™˜

        Returns:
            í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        stats = {
            'parser_stats': self.parser.get_parsing_stats(),
            'qdrant_available': self.doc_store is not None and self.table_store is not None
        }

        if self.doc_store and self.table_store:
            try:
                stats['tcfd_documents'] = self.doc_store.get_collection_info()
                stats['tcfd_tables'] = self.table_store.get_collection_info()
            except Exception as e:
                logger.warning(f"Failed to get Qdrant stats: {e}")
                stats['qdrant_error'] = str(e)

        return stats

    def ingest_multiple_pdfs(
        self,
        pdf_configs: List[Dict[str, Any]],
        force_reparse: bool = False
    ) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ PDFë¥¼ ë°°ì¹˜ë¡œ ìˆ˜ì§‘

        Args:
            pdf_configs: PDF ì„¤ì • ë¦¬ìŠ¤íŠ¸
                [
                    {
                        'file_path': 'path/to/file.pdf',
                        'metadata': {...}
                    },
                    ...
                ]
            force_reparse: ìºì‹œ ë¬´ì‹œ ì—¬ë¶€

        Returns:
            ìˆ˜ì§‘ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []

        for config in pdf_configs:
            try:
                result = self.ingest_pdf(
                    file_path=config['file_path'],
                    document_metadata=config['metadata'],
                    force_reparse=force_reparse
                )
                results.append(result)
            except Exception as e:
                logger.error(f"Failed to ingest {config['file_path']}: {e}")
                results.append({
                    'file_path': config['file_path'],
                    'error': str(e),
                    'status': 'failed'
                })

        return results


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # RAG Ingestion Service ì´ˆê¸°í™”
    service = RAGIngestionService()

    # í†µê³„ í™•ì¸
    stats = service.get_ingestion_stats()
    print("\nğŸ“Š RAG Ingestion Statistics:")
    print(json.dumps(stats, indent=2, ensure_ascii=False))

    # ì‹¤ì œ ìˆ˜ì§‘ ì˜ˆì‹œ (ì£¼ì˜: ì‹¤í–‰í•˜ì§€ ë§ ê²ƒ!)
    # result = service.ingest_pdf(
    #     file_path="ê°ì¢… ìë£Œ/For_RAG/FINAL-2017-TCFD-Report.pdf",
    #     document_metadata={
    #         'document_id': 'tcfd_official_2017',
    #         'document_type': 'guideline',
    #         'source': 'TCFD',
    #         'year': 2017,
    #         'title': 'Final Report: Recommendations of the TCFD',
    #         'tags': ['tcfd', 'governance', 'strategy', 'risk_management', 'metrics']
    #     }
    # )
