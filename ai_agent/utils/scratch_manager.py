'''
ÌååÏùºÎ™Ö: scratch_manager.py
ÏµúÏ¢Ö ÏàòÏ†ïÏùº: 2025-11-21
Î≤ÑÏ†Ñ: v01
ÌååÏùº Í∞úÏöî: TTL Í∏∞Î∞ò Scratch Space Í¥ÄÎ¶¨Ïûê (ÏûÑÏãú Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• Î∞è ÏûêÎèô Ï†ïÎ¶¨)
Î≥ÄÍ≤Ω Ïù¥Î†•:
	- 2025-11-21: v01 - Ï¥àÍ∏∞ ÏÉùÏÑ± (Ephemeral Disk Cache with TTL Eviction)
'''
from pathlib import Path
import json
import shutil
from datetime import datetime, timedelta
from typing import Optional, Any, Dict
import uuid
import logging


class ScratchSpaceManager:
	"""
	TTL Í∏∞Î∞ò Scratch Space Í¥ÄÎ¶¨Ïûê

	ÎåÄÏö©Îüâ Í∏∞ÌõÑ Îç∞Ïù¥ÌÑ∞Î•º ÏûÑÏãúÎ°ú ÎîîÏä§ÌÅ¨Ïóê Ï†ÄÏû•ÌïòÍ≥†,
	TTL(Time To Live) Í∏∞Î∞òÏúºÎ°ú ÏûêÎèô Ï†ïÎ¶¨ÌïòÎäî ÏãúÏä§ÌÖú

	ÌäπÏßï:
	- ÏÑ∏ÏÖòÎ≥Ñ Í≤©Î¶¨Îêú ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±
	- Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò TTL Í¥ÄÎ¶¨
	- Îã§ÏñëÌïú Ìè¨Îß∑ ÏßÄÏõê (JSON, NumPy, Pickle, NetCDF)
	- ÏûêÎèô ÎßåÎ£å Î∞è Ï†ïÎ¶¨

	ÏÇ¨Ïö© ÏòàÏãú:
		manager = ScratchSpaceManager()
		session_id = manager.create_session(ttl_hours=4)
		manager.save_data(session_id, "climate.json", data, format="json")
		data = manager.load_data(session_id, "climate.json", format="json")
		manager.cleanup_expired()
	"""

	def __init__(self, base_path: str = "./scratch", default_ttl_hours: int = 24):
		"""
		ScratchSpaceManager Ï¥àÍ∏∞Ìôî

		Args:
			base_path: Scratch Space Í∏∞Î≥∏ Í≤ΩÎ°ú (Í∏∞Î≥∏Í∞í: "./scratch")
			default_ttl_hours: Í∏∞Î≥∏ TTL ÏãúÍ∞Ñ (Í∏∞Î≥∏Í∞í: 24ÏãúÍ∞Ñ)
		"""
		self.base_path = Path(base_path)
		self.base_path.mkdir(exist_ok=True)
		self.default_ttl = timedelta(hours=default_ttl_hours)
		self.logger = logging.getLogger(__name__)

		# .gitignore ÌååÏùº ÏÉùÏÑ± (scratch ÎîîÎ†âÌÜ†Î¶¨Îäî git Ï∂îÏ†Å Ïïà Ìï®)
		gitignore_path = self.base_path / ".gitignore"
		if not gitignore_path.exists():
			with open(gitignore_path, "w") as f:
				f.write("*\n!.gitignore\n")

	def create_session(
		self,
		session_id: Optional[str] = None,
		ttl_hours: Optional[int] = None,
		metadata: Optional[Dict[str, Any]] = None
	) -> str:
		"""
		ÏÉà Î∂ÑÏÑù ÏÑ∏ÏÖò ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±

		Args:
			session_id: ÏÑ∏ÏÖò ID (Í∏∞Î≥∏Í∞í: ÏûêÎèô ÏÉùÏÑ± UUID)
			ttl_hours: TTL ÏãúÍ∞Ñ (Í∏∞Î≥∏Í∞í: default_ttl_hours)
			metadata: Ï∂îÍ∞Ä Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ (ÏÑ†ÌÉù)

		Returns:
			ÏÉùÏÑ±Îêú ÏÑ∏ÏÖò ID
		"""
		session_id = session_id or str(uuid.uuid4())[:8]
		session_path = self.base_path / f"run_{session_id}"
		session_path.mkdir(exist_ok=True)

		# Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†ÄÏû•
		ttl = timedelta(hours=ttl_hours) if ttl_hours else self.default_ttl
		now = datetime.now()
		expires_at = now + ttl

		session_metadata = {
			"session_id": session_id,
			"created_at": now.isoformat(),
			"expires_at": expires_at.isoformat(),
			"ttl_hours": ttl_hours or (self.default_ttl.total_seconds() / 3600),
			"files": [],
			"status": "active"
		}

		# ÏÇ¨Ïö©Ïûê Ï†ïÏùò Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î≥ëÌï©
		if metadata:
			session_metadata.update(metadata)

		with open(session_path / "metadata.json", "w") as f:
			json.dump(session_metadata, f, indent=2, ensure_ascii=False)

		self.logger.info(f"Created scratch session: {session_id} (TTL: {ttl_hours or self.default_ttl.total_seconds() / 3600}h, Expires: {expires_at})")

		return session_id

	def get_session_path(self, session_id: str) -> Path:
		"""
		ÏÑ∏ÏÖò ÎîîÎ†âÌÜ†Î¶¨ Í≤ΩÎ°ú Î∞òÌôò

		Args:
			session_id: ÏÑ∏ÏÖò ID

		Returns:
			ÏÑ∏ÏÖò ÎîîÎ†âÌÜ†Î¶¨ Í≤ΩÎ°ú
		"""
		return self.base_path / f"run_{session_id}"

	def session_exists(self, session_id: str) -> bool:
		"""
		ÏÑ∏ÏÖò Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏

		Args:
			session_id: ÏÑ∏ÏÖò ID

		Returns:
			ÏÑ∏ÏÖò Ï°¥Ïû¨ Ïó¨Î∂Ä
		"""
		return self.get_session_path(session_id).exists()

	def get_metadata(self, session_id: str) -> Dict[str, Any]:
		"""
		ÏÑ∏ÏÖò Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï°∞Ìöå

		Args:
			session_id: ÏÑ∏ÏÖò ID

		Returns:
			Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÎîïÏÖîÎÑàÎ¶¨
		"""
		metadata_file = self.get_session_path(session_id) / "metadata.json"

		if not metadata_file.exists():
			raise FileNotFoundError(f"Session metadata not found: {session_id}")

		with open(metadata_file) as f:
			return json.load(f)

	def update_metadata(self, session_id: str, updates: Dict[str, Any]):
		"""
		ÏÑ∏ÏÖò Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏

		Args:
			session_id: ÏÑ∏ÏÖò ID
			updates: ÏóÖÎç∞Ïù¥Ìä∏Ìï† Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÎîïÏÖîÎÑàÎ¶¨
		"""
		metadata = self.get_metadata(session_id)
		metadata.update(updates)

		metadata_file = self.get_session_path(session_id) / "metadata.json"
		with open(metadata_file, "w") as f:
			json.dump(metadata, f, indent=2, ensure_ascii=False)

	def save_data(
		self,
		session_id: str,
		filename: str,
		data: Any,
		format: str = "json"
	) -> str:
		"""
		Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•

		Args:
			session_id: ÏÑ∏ÏÖò ID
			filename: ÌååÏùºÎ™Ö
			data: Ï†ÄÏû•Ìï† Îç∞Ïù¥ÌÑ∞
			format: Ìè¨Îß∑ ("json", "numpy", "pickle", "netcdf")

		Returns:
			Ï†ÄÏû•Îêú ÌååÏùº Í≤ΩÎ°ú
		"""
		session_path = self.get_session_path(session_id)

		if not session_path.exists():
			raise FileNotFoundError(f"Session not found: {session_id}")

		file_path = session_path / filename

		try:
			if format == "json":
				with open(file_path, "w", encoding="utf-8") as f:
					json.dump(data, f, indent=2, ensure_ascii=False)

			elif format == "numpy":
				import numpy as np
				np.save(file_path, data)

			elif format == "pickle":
				import pickle
				with open(file_path, "wb") as f:
					pickle.dump(data, f)

			elif format == "netcdf":
				# xarray ÎòêÎäî netCDF4 ÏÇ¨Ïö©
				if hasattr(data, 'to_netcdf'):
					data.to_netcdf(file_path)
				else:
					raise ValueError("Data must be xarray.Dataset or xarray.DataArray for netcdf format")

			else:
				raise ValueError(f"Unsupported format: {format}")

			# Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏ (ÌååÏùº Î™©Î°ù Ï∂îÍ∞Ä)
			metadata = self.get_metadata(session_id)
			if "files" not in metadata:
				metadata["files"] = []

			file_info = {
				"filename": filename,
				"format": format,
				"saved_at": datetime.now().isoformat(),
				"size_bytes": file_path.stat().st_size
			}

			# Ï§ëÎ≥µ Ï†úÍ±∞ (Í∞ôÏùÄ ÌååÏùºÎ™ÖÏù¥Î©¥ ÎçÆÏñ¥Ïì∞Í∏∞)
			metadata["files"] = [f for f in metadata["files"] if f["filename"] != filename]
			metadata["files"].append(file_info)

			self.update_metadata(session_id, metadata)

			self.logger.debug(f"Saved {filename} ({format}) to session {session_id}")

			return str(file_path)

		except Exception as e:
			self.logger.error(f"Failed to save {filename} to session {session_id}: {e}")
			raise

	def load_data(
		self,
		session_id: str,
		filename: str,
		format: str = "json"
	) -> Any:
		"""
		Îç∞Ïù¥ÌÑ∞ Î°úÎìú

		Args:
			session_id: ÏÑ∏ÏÖò ID
			filename: ÌååÏùºÎ™Ö
			format: Ìè¨Îß∑ ("json", "numpy", "pickle", "netcdf")

		Returns:
			Î°úÎìúÎêú Îç∞Ïù¥ÌÑ∞
		"""
		file_path = self.get_session_path(session_id) / filename

		if not file_path.exists():
			raise FileNotFoundError(f"File not found: {file_path}")

		try:
			if format == "json":
				with open(file_path, encoding="utf-8") as f:
					return json.load(f)

			elif format == "numpy":
				import numpy as np
				return np.load(file_path, allow_pickle=True)

			elif format == "pickle":
				import pickle
				with open(file_path, "rb") as f:
					return pickle.load(f)

			elif format == "netcdf":
				import xarray as xr
				return xr.open_dataset(file_path)

			else:
				raise ValueError(f"Unsupported format: {format}")

		except Exception as e:
			self.logger.error(f"Failed to load {filename} from session {session_id}: {e}")
			raise

	def list_files(self, session_id: str) -> list:
		"""
		ÏÑ∏ÏÖòÏùò ÌååÏùº Î™©Î°ù Ï°∞Ìöå

		Args:
			session_id: ÏÑ∏ÏÖò ID

		Returns:
			ÌååÏùº Ï†ïÎ≥¥ Î¶¨Ïä§Ìä∏
		"""
		metadata = self.get_metadata(session_id)
		return metadata.get("files", [])

	def cleanup_expired(self, dry_run: bool = False) -> int:
		"""
		TTL ÎßåÎ£åÎêú ÏÑ∏ÏÖò Ï†ïÎ¶¨

		Args:
			dry_run: TrueÏù¥Î©¥ Ïã§Ï†ú ÏÇ≠Ï†ú ÏóÜÏù¥ Î™©Î°ùÎßå Î∞òÌôò (Í∏∞Î≥∏Í∞í: False)

		Returns:
			Ï†ïÎ¶¨Îêú ÏÑ∏ÏÖò Í∞úÏàò
		"""
		now = datetime.now()
		cleaned = 0

		for session_dir in self.base_path.glob("run_*"):
			metadata_file = session_dir / "metadata.json"

			if not metadata_file.exists():
				self.logger.warning(f"Metadata not found for session: {session_dir.name}, skipping")
				continue

			try:
				with open(metadata_file) as f:
					metadata = json.load(f)

				expires_at = datetime.fromisoformat(metadata["expires_at"])

				if now > expires_at:
					if dry_run:
						self.logger.info(f"[DRY RUN] Would clean expired session: {session_dir.name} (expired: {expires_at})")
					else:
						shutil.rmtree(session_dir)
						self.logger.info(f"üóëÔ∏è  Cleaned expired session: {session_dir.name} (expired: {expires_at})")

					cleaned += 1

			except Exception as e:
				self.logger.error(f"Error processing session {session_dir.name}: {e}")
				continue

		if cleaned > 0:
			self.logger.info(f"Cleanup complete: {cleaned} session(s) removed")

		return cleaned

	def cleanup_session(self, session_id: str):
		"""
		ÌäπÏ†ï ÏÑ∏ÏÖò Ï¶âÏãú ÏÇ≠Ï†ú

		Args:
			session_id: ÏÑ∏ÏÖò ID
		"""
		session_path = self.get_session_path(session_id)

		if session_path.exists():
			shutil.rmtree(session_path)
			self.logger.info(f"üóëÔ∏è  Deleted session: {session_id}")
		else:
			self.logger.warning(f"Session not found: {session_id}")

	def get_stats(self) -> Dict[str, Any]:
		"""
		Scratch Space ÌÜµÍ≥Ñ Ï°∞Ìöå

		Returns:
			ÌÜµÍ≥Ñ Ï†ïÎ≥¥ ÎîïÏÖîÎÑàÎ¶¨
		"""
		sessions = list(self.base_path.glob("run_*"))
		total_sessions = len(sessions)

		total_size = 0
		active_sessions = 0
		expired_sessions = 0
		now = datetime.now()

		for session_dir in sessions:
			metadata_file = session_dir / "metadata.json"

			if metadata_file.exists():
				try:
					with open(metadata_file) as f:
						metadata = json.load(f)

					expires_at = datetime.fromisoformat(metadata["expires_at"])

					if now > expires_at:
						expired_sessions += 1
					else:
						active_sessions += 1
				except:
					pass

			# ÎîîÎ†âÌÜ†Î¶¨ ÌÅ¨Í∏∞ Í≥ÑÏÇ∞
			for file_path in session_dir.rglob("*"):
				if file_path.is_file():
					total_size += file_path.stat().st_size

		return {
			"total_sessions": total_sessions,
			"active_sessions": active_sessions,
			"expired_sessions": expired_sessions,
			"total_size_bytes": total_size,
			"total_size_mb": round(total_size / (1024 * 1024), 2),
			"base_path": str(self.base_path)
		}
