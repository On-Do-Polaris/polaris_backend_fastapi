'''
íŒŒì¼ëª…: additional_data_agent.py
ì‘ì„±ì¼: 2025-12-16
ë²„ì „: v05 (DB ì¡°íšŒë§Œ - Excel ì§ì ‘ ì ‘ê·¼ X)
íŒŒì¼ ê°œìš”: ì¶”ê°€ ë°ì´í„° ë¶„ì„ ì—ì´ì „íŠ¸ (ë³´ê³ ì„œ ìƒì„±ìš© ê°€ì´ë“œë¼ì¸ ì œê³µ)

ì—­í• :
    - AdditionalDataLoaderë¥¼ í†µí•´ DBì—ì„œë§Œ ì¶”ê°€ ë°ì´í„° ì¡°íšŒ (Excel ì§ì ‘ ì ‘ê·¼ X)
    - ì¶”ì¶œëœ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë³´ê³ ì„œ ìƒì„± ì—ì´ì „íŠ¸ë¥¼ ìœ„í•œ ê°€ì´ë“œë¼ì¸ ìƒì„±
    - âš ï¸ ì¡°ê±´ë¶€ ì‹¤í–‰: ì¶”ê°€ ë°ì´í„°ê°€ DBì— ì ì¬ëœ ê²½ìš°ì—ë§Œ ì‹¤í–‰
    - ë‹¤ì¤‘ ì‚¬ì—…ì¥ ë³‘ë ¬ ì²˜ë¦¬ ì§€ì› (asyncio.gather)

ì•„í‚¤í…ì²˜:
    - additional_data_loader.py: ETL (Excel â†’ DB ì ì¬) â† ë³„ë„ íŠ¸ë¦¬ê±°ë¡œ ì‹¤í–‰
    - additional_data_agent.py: ë¶„ì„ (DB ì¡°íšŒë§Œ â†’ LLM â†’ ê°€ì´ë“œë¼ì¸) â† Node 0ì—ì„œ í˜¸ì¶œ

ë³€ê²½ ì´ë ¥:
    - 2025-12-14: v01 - ì´ˆê¸° ìƒì„± (TCFD Report v2 Refactoring)
    - 2025-12-15: v02 - ë‹¤ì¤‘ ì‚¬ì—…ì¥ ë°°ì¹˜ ì²˜ë¦¬ í™•ì¸, TCFD Report v2.1 ëŒ€ì‘
    - 2025-12-15: v03 - ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ (asyncio.gather, ì „ì²´ async ì „í™˜)
    - 2025-12-16: v04 - ETL ë¶„ë¦¬ (AdditionalDataLoader ì‚¬ìš©)
    - 2025-12-16: v05 - DB ì¡°íšŒë§Œ í•˜ë„ë¡ ìˆ˜ì • (Excel ì§ì ‘ ì ‘ê·¼ X)
'''

from typing import Dict, Any, List, Optional
import logging
from datetime import datetime
import json
import asyncio

# AdditionalDataLoader ì„í¬íŠ¸ (ETL ë‹´ë‹¹)
try:
    from .additional_data_loader import AdditionalDataLoader
except ImportError:
    AdditionalDataLoader = None
    print("âš ï¸ AdditionalDataLoaderë¥¼ ì„í¬íŠ¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

logger = logging.getLogger(__name__)


class AdditionalDataAgent:
    """
    ì¶”ê°€ ë°ì´í„° ë¶„ì„ ì—ì´ì „íŠ¸ (DB â†’ LLM Guideline)
    â†’ v05: DB ì¡°íšŒë§Œ (Excel ì§ì ‘ ì ‘ê·¼ X)

    ì…ë ¥:
        - site_ids: List[str] (ë¶„ì„ ëŒ€ìƒ ì‚¬ì—…ì¥ UUID ë¦¬ìŠ¤íŠ¸)

    ì¶œë ¥:
        - site_specific_guidelines: Dict[str, Dict] (ì‚¬ì—…ì¥ë³„ ê°€ì´ë“œë¼ì¸)
        - summary: str (ì „ì²´ ìš”ì•½)

    ì•„í‚¤í…ì²˜:
        - AdditionalDataLoader: ETL (Excel â†’ DB) - ë³„ë„ íŠ¸ë¦¬ê±°
        - AdditionalDataAgent: ë¶„ì„ (DB ì¡°íšŒë§Œ â†’ LLM â†’ ê°€ì´ë“œë¼ì¸) - Node 0 í˜¸ì¶œ
    """

    def __init__(self, llm_client=None, db_url: Optional[str] = None):
        """
        ì´ˆê¸°í™”
        :param llm_client: LLM í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ (í…ìŠ¤íŠ¸ ìƒì„±ìš©)
        :param db_url: Datawarehouse DB URL (site_additional_data í…Œì´ë¸” ì ‘ê·¼ìš©)
        """
        self.logger = logger
        self.llm_client = llm_client

        # AdditionalDataLoader ì´ˆê¸°í™” (DB ì¡°íšŒìš©)
        if AdditionalDataLoader:
            try:
                self.data_loader = AdditionalDataLoader(db_url=db_url)
                self.logger.info("AdditionalDataLoader ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                self.logger.error(f"AdditionalDataLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.data_loader = None
        else:
            self.data_loader = None

        self.logger.info("AdditionalDataAgent ì´ˆê¸°í™” ì™„ë£Œ")

    async def analyze_from_db(self, site_ids: List[str]) -> Dict[str, Any]:
        """
        DBì—ì„œ ì¶”ê°€ ë°ì´í„° ì¡°íšŒ ë° ê°€ì´ë“œë¼ì¸ ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬)
        â†’ Node 0ì—ì„œ í˜¸ì¶œí•˜ëŠ” ì£¼ ë©”ì„œë“œ

        :param site_ids: ë¶„ì„ ëŒ€ìƒ ì‚¬ì—…ì¥ UUID ë¦¬ìŠ¤íŠ¸
        :return: ë¶„ì„ ê²°ê³¼ (ì‚¬ì—…ì¥ë³„ ê°€ì´ë“œë¼ì¸ + ì „ì²´ ìš”ì•½)
        """
        self.logger.info(f"ì¶”ê°€ ë°ì´í„° ë¶„ì„ ì‹œì‘ (DB ì¡°íšŒ): {len(site_ids)}ê°œ ì‚¬ì—…ì¥")

        try:
            # 1. ê° ì‚¬ì—…ì¥ë³„ë¡œ DBì—ì„œ ì¶”ê°€ ë°ì´í„° ì¡°íšŒ
            site_data = {}
            for site_id in site_ids:
                if self.data_loader:
                    data = self.data_loader.fetch_all_for_site(site_id)
                    site_data[site_id] = data
                else:
                    self.logger.warning(f"DataLoader ì—†ìŒ, ì‚¬ì—…ì¥ {site_id} ë¹ˆ ë°ì´í„°")
                    site_data[site_id] = {}

            # 2. ê° ì‚¬ì—…ì¥ë³„ ê°€ì´ë“œë¼ì¸ ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬)
            tasks = [
                self._generate_site_guideline(site_id, data)
                for site_id, data in site_data.items()
            ]

            self.logger.info(f"ğŸ”„ {len(tasks)}ê°œ ì‚¬ì—…ì¥ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘")
            guidelines_list = await asyncio.gather(*tasks)

            # ê²°ê³¼ë¥¼ dictë¡œ ë³€í™˜
            site_specific_guidelines = {
                site_id: guideline
                for site_id, guideline in zip(site_data.keys(), guidelines_list)
            }
            self.logger.info(f"âœ… {len(site_specific_guidelines)}ê°œ ì‚¬ì—…ì¥ ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ")

            # 3. ì „ì²´ ìš”ì•½ (Optional)
            summary = await self._generate_summary(site_specific_guidelines)

            result = {
                "meta": {
                    "analyzed_at": datetime.now().isoformat(),
                    "source": "database",
                    "site_count": len(site_specific_guidelines)
                },
                "site_specific_guidelines": site_specific_guidelines,
                "summary": summary,
                "status": "completed"
            }

            self.logger.info("ì¶”ê°€ ë°ì´í„° ë¶„ì„ ì™„ë£Œ (DB ì¡°íšŒ)")
            return result

        except Exception as e:
            self.logger.error(f"ì¶”ê°€ ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "meta": {
                    "analyzed_at": datetime.now().isoformat(),
                    "source": "database",
                    "error": str(e)
                },
                "site_specific_guidelines": {},
                "summary": "",
                "status": "failed"
            }

    async def analyze(self, excel_file: str = None, site_ids: List = None) -> Dict[str, Any]:
        """
        ì¶”ê°€ ë°ì´í„° ë¶„ì„ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)

        âš ï¸ DEPRECATED: ìƒˆë¡œìš´ ì½”ë“œì—ì„œëŠ” analyze_from_db() ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.

        :param excel_file: Excel íŒŒì¼ ê²½ë¡œ (ë¯¸ì‚¬ìš©, í˜¸í™˜ì„± ìœ ì§€)
        :param site_ids: ë¶„ì„ ëŒ€ìƒ ì‚¬ì—…ì¥ ID ë¦¬ìŠ¤íŠ¸ (str UUID ë˜ëŠ” int)
        :return: ë¶„ì„ ê²°ê³¼ (ì‚¬ì—…ì¥ë³„ ê°€ì´ë“œë¼ì¸ + ì „ì²´ ìš”ì•½)
        """
        self.logger.warning("analyze()ëŠ” deprecatedì…ë‹ˆë‹¤. analyze_from_db()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")

        # site_idsë¥¼ strë¡œ ë³€í™˜
        if site_ids:
            site_ids_str = [str(sid) for sid in site_ids]
        else:
            site_ids_str = []

        # DB ì¡°íšŒ ë©”ì„œë“œë¡œ ìœ„ì„
        return await self.analyze_from_db(site_ids_str)

    async def _generate_site_guideline(self, site_id: int, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì‚¬ì—…ì¥ë³„ ê°€ì´ë“œë¼ì¸ ìƒì„± (LLM í™œìš© - ë¹„ë™ê¸°)
        """
        if not data:
            return {
                "site_id": site_id,
                "guideline": "ì¶”ê°€ ë°ì´í„° ì—†ìŒ",
                "relevance": 0.0,
                "key_insights": []
            }

        # LLM ì‚¬ìš©
        if self.llm_client:
            try:
                prompt = self._build_prompt(site_id, data)

                # ë¹„ë™ê¸° LLM í˜¸ì¶œ
                if hasattr(self.llm_client, 'ainvoke'):
                    response = await self.llm_client.ainvoke(prompt)
                else:
                    # Fallback to sync invoke
                    response = self.llm_client.invoke(prompt)

                # AIMessageì—ì„œ content ì¶”ì¶œ
                guideline_text = response.content if hasattr(response, 'content') else str(response)

                # ê°„ë‹¨í•œ íŒŒì‹± (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒŒì‹± í•„ìš”)
                return {
                    "site_id": site_id,
                    "guideline": guideline_text,
                    "key_insights": self._extract_key_insights(guideline_text)
                }
            except Exception as e:
                self.logger.error(f"LLM ê°€ì´ë“œë¼ì¸ ìƒì„± ì‹¤íŒ¨ (ì‚¬ì—…ì¥ {site_id}): {e}")
                return self._generate_fallback_guideline(site_id, data)

        return self._generate_fallback_guideline(site_id, data)

    def _extract_key_insights(self, guideline_text: str) -> List[str]:
        """
        ê°€ì´ë“œë¼ì¸ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ

        âš ï¸ ê°„ë‹¨í•œ êµ¬í˜„: ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
        ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒŒì‹± í•„ìš” (ì •ê·œí‘œí˜„ì‹, LLM ì¬í˜¸ì¶œ ë“±)
        """
        # ê°„ë‹¨í•œ íŒŒì‹±: "- "ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ë§Œ ì¶”ì¶œ
        insights = []
        for line in guideline_text.split('\n'):
            line = line.strip()
            if line.startswith('- '):
                insights.append(line[2:])  # "- " ì œê±°

        return insights[:5]  # ìµœëŒ€ 5ê°œë§Œ ë°˜í™˜

    def _generate_fallback_guideline(self, site_id: int, data: Any) -> Dict[str, Any]:
        """LLM ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê°€ì´ë“œë¼ì¸ ìƒì„±"""
        guideline = f"## ì‚¬ì—…ì¥ {site_id} ì¶”ê°€ ì •ë³´\n\n"
        key_insights = []

        if data:
            # dataê°€ listì¸ ê²½ìš° (site_id ì»¬ëŸ¼ ì—†ëŠ” Excelì—ì„œ ì „ì²´ ë°ì´í„°)
            if isinstance(data, list):
                if len(data) > 0:
                    # ì²« ë²ˆì§¸ í–‰ì˜ ì»¬ëŸ¼ë“¤ í‘œì‹œ
                    first_row = data[0]
                    columns = list(first_row.keys()) if isinstance(first_row, dict) else []
                    guideline += f"- ë°ì´í„° ì»¬ëŸ¼: {', '.join(columns)}\n"
                    guideline += f"- ì´ ë ˆì½”ë“œ ìˆ˜: {len(data)}í–‰\n"

                    # ì‹œê³„ì—´ ë°ì´í„° ìš”ì•½
                    if columns:
                        key_insights.append(f"ì‹œê³„ì—´ ë°ì´í„° {len(data)}ê°œ ë ˆì½”ë“œ")
                        for col in columns[:3]:  # ìµœëŒ€ 3ê°œ ì»¬ëŸ¼ë§Œ í‘œì‹œ
                            key_insights.append(f"{col} ë°ì´í„° í¬í•¨")
            # dataê°€ dictì¸ ê²½ìš° (ê¸°ì¡´ ë¡œì§)
            elif isinstance(data, dict):
                for key, value in data.items():
                    if value and str(value).strip():
                        guideline += f"- {key}: {value}\n"
            else:
                guideline += f"- ë°ì´í„° íƒ€ì…: {type(data).__name__}\n"
        else:
            guideline += "- ì¶”ê°€ ë°ì´í„° ì—†ìŒ\n"

        return {
            "site_id": site_id,
            "guideline": guideline,
            "key_insights": key_insights
        }

    def _build_prompt(self, site_id: int, data: Dict[str, Any]) -> str:
        """
        LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì¶”ê°€ ë°ì´í„° â†’ ê°€ì´ë“œë¼ì¸ ë³€í™˜)
        """
        # datetime ì§ë ¬í™” í•¸ë“¤ëŸ¬
        def json_serializer(obj):
            if hasattr(obj, 'isoformat'):
                return obj.isoformat()
            raise TypeError(f"Object of type {type(obj)} is not JSON serializable")

        # ë°ì´í„° ìš”ì•½ (í† í° ì œí•œ ë•Œë¬¸ì— ì „ì²´ ë°ì´í„° ëŒ€ì‹  ìš”ì•½ë§Œ)
        summarized_data = self._summarize_data_for_prompt(data)
        data_json = json.dumps(summarized_data, indent=2, ensure_ascii=False, default=json_serializer)

        prompt = f"""ë‹¹ì‹ ì€ TCFD ë³´ê³ ì„œ ìƒì„± ì „ë¬¸ê°€ì´ë©°, **ì‚¬ìš©ìê°€ ì œê³µí•œ ì¶”ê°€ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë³´ê³ ì„œ ìƒì„± ì—ì´ì „íŠ¸ë¥¼ ìœ„í•œ ê°€ì´ë“œë¼ì¸**ì„ ì‘ì„±í•˜ëŠ” ì—­í• ì„ ë§¡ê³  ìˆìŠµë‹ˆë‹¤.

ì œê³µëœ ë°ì´í„°ëŠ” **ì‚¬ì—…ì¥ {site_id}ì— ëŒ€í•œ ì¶”ê°€ ì •ë³´**ì´ë©°, ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë³´ê³ ì„œ ì‘ì„± ì‹œ í™œìš©í•  í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ì •ë¦¬í•´ì£¼ì„¸ìš”.

âš ï¸ **ì¤‘ìš”**: ì´ ê°€ì´ë“œë¼ì¸ì€ ì¶”í›„ Node 2-A (Scenario Analysis), Node 2-B (Impact Analysis), Node 2-C (Mitigation Strategies) ì—ì´ì „íŠ¸ê°€ ì°¸ê³ í•©ë‹ˆë‹¤.

---
## ì‚¬ì—…ì¥ {site_id} ì¶”ê°€ ë°ì´í„°

{data_json}

---
## ê°€ì´ë“œë¼ì¸ ì‘ì„± ì§€ì¹¨

ìœ„ì˜ ì¶”ê°€ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ëª©ì°¨ì— ë”°ë¼ **ë³´ê³ ì„œ ìƒì„± ì—ì´ì „íŠ¸ë¥¼ ìœ„í•œ ê°€ì´ë“œë¼ì¸**ì„ ì‘ì„±í•˜ì„¸ìš”.

**[ê°€ì´ë“œë¼ì¸ ëª©ì°¨]**
1. **ë°ì´í„° ìš”ì•½** (3-5ë¬¸ì¥)
   - ì œê³µëœ ì¶”ê°€ ë°ì´í„°ì˜ í•µì‹¬ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½
   - ì–´ë–¤ ìœ í˜•ì˜ ì •ë³´ì¸ì§€ ëª…ì‹œ (ì˜ˆ: ì‹œì„¤ë¬¼ ì„¸ë¶€ ì •ë³´, ìš´ì˜ í˜„í™©, ì¬ë¬´ ë°ì´í„° ë“±)

2. **ë³´ê³ ì„œ í™œìš© ë°©ì•ˆ**
   - Node 2-A (Scenario Analysis): ì´ ë°ì´í„°ê°€ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„ì— ì–´ë–»ê²Œ í™œìš©ë  ìˆ˜ ìˆëŠ”ì§€
   - Node 2-B (Impact Analysis): ì˜í–¥ ë¶„ì„ ì‹œ ê°•ì¡°í•´ì•¼ í•  í¬ì¸íŠ¸
   - Node 2-C (Mitigation Strategies): ëŒ€ì‘ ì „ëµ ìˆ˜ë¦½ ì‹œ ì°¸ê³ í•  ì •ë³´

3. **ì£¼ì˜ì‚¬í•­**
   - ì´ ë°ì´í„°ë¥¼ ê³¼ë„í•˜ê²Œ ì¼ë°˜í™”í•˜ê±°ë‚˜ ì™œê³¡í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜
   - íŠ¹ì • ì‚¬ì—…ì¥ì—ë§Œ í•´ë‹¹í•˜ëŠ” ì •ë³´ì„ì„ ëª…ì‹œ

**í†¤ì•¤ë§¤ë„ˆ**: ê°„ê²°í•˜ê³  ì‹¤ìš©ì ì¸ ì–´ì¡°ë¡œ, ë³´ê³ ì„œ ìƒì„± ì—ì´ì „íŠ¸ê°€ ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
**ì£¼ì˜**: ìµœì¢… ë³´ê³ ì„œ ë‚´ìš©ì„ ì§ì ‘ ì‘ì„±í•˜ì§€ ë§ˆì„¸ìš”. ê°€ì´ë“œë¼ì¸ê³¼ í•µì‹¬ í¬ì¸íŠ¸ë§Œ ì œê³µí•˜ì„¸ìš”.
"""
        return prompt

    def _summarize_data_for_prompt(self, data: Dict[str, Any], max_rows_per_sheet: int = 20) -> Dict[str, Any]:
        """
        LLM í† í° ì œí•œì„ ìœ„í•´ ë°ì´í„° ìš”ì•½ (ì „ì²´ ë¤í”„ ëŒ€ì‹  ìƒ˜í”Œë§Œ)

        Args:
            data: ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° (fetch_all_for_site ê²°ê³¼)
            max_rows_per_sheet: ì‹œíŠ¸ë‹¹ ìµœëŒ€ í–‰ ìˆ˜

        Returns:
            ìš”ì•½ëœ ë°ì´í„° (ë©”íƒ€ ì •ë³´ + ìƒ˜í”Œ í–‰)
        """
        summarized = {}

        for category, items in data.items():
            summarized[category] = []

            for item in items:
                file_info = {
                    "file_name": item.get("file_name", "unknown"),
                    "category": category,
                    "uploaded_at": str(item.get("uploaded_at", "")),
                }

                # structured_dataì—ì„œ ìƒ˜í”Œë§Œ ì¶”ì¶œ
                structured = item.get("structured_data", {})
                if isinstance(structured, dict):
                    sheets_summary = []
                    for sheet in structured.get("sheets", []):
                        sheet_name = sheet.get("name", "Sheet")
                        row_count = sheet.get("row_count", 0)
                        content = sheet.get("content", "")

                        # ì²˜ìŒ Ní–‰ë§Œ ì¶”ì¶œ
                        lines = content.split("\n")[:max_rows_per_sheet]
                        sample_content = "\n".join(lines)

                        sheets_summary.append({
                            "sheet_name": sheet_name,
                            "total_rows": row_count,
                            "sample_rows": len(lines),
                            "sample_content": sample_content
                        })

                    file_info["sheets"] = sheets_summary
                else:
                    file_info["data_preview"] = str(structured)[:2000]

                summarized[category].append(file_info)

        return summarized

    async def _generate_summary(self, site_specific_guidelines: Dict[int, Dict[str, Any]]) -> str:
        """
        ì „ì²´ ì‚¬ì—…ì¥ ê°€ì´ë“œë¼ì¸ ìš”ì•½ (ë¹„ë™ê¸°)
        """
        if not site_specific_guidelines:
            return "ì¶”ê°€ ë°ì´í„° ì—†ìŒ"

        summary = f"## ì¶”ê°€ ë°ì´í„° ì „ì²´ ìš”ì•½\n\n"
        summary += f"ì´ {len(site_specific_guidelines)}ê°œ ì‚¬ì—…ì¥ì— ëŒ€í•œ ì¶”ê°€ ë°ì´í„°ê°€ ì œê³µë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"

        # ì‚¬ì—…ì¥ë³„ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ìˆ˜ ì§‘ê³„
        total_insights = sum(len(g.get('key_insights', [])) for g in site_specific_guidelines.values())
        summary += f"ì´ {total_insights}ê°œì˜ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ê°€ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.\n"

        return summary
