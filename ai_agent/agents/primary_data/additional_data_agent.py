'''
íŒŒì¼ëª…: additional_data_agent.py
ì‘ì„±ì¼: 2025-12-17
ë²„ì „: v08 (Concrete Quantitative Data - êµ¬ì²´ì ì¸ ì •ëŸ‰ ë°ì´í„° ì¶”ì¶œ ê°•í™”)
íŒŒì¼ ê°œìš”: ì¶”ê°€ ë°ì´í„° ë¶„ì„ ì—ì´ì „íŠ¸ (ë³´ê³ ì„œ ìƒì„±ìš© ê°€ì´ë“œë¼ì¸ ì œê³µ)

ì—­í• :
    - AdditionalDataLoaderë¥¼ í†µí•´ DBì—ì„œë§Œ ì¶”ê°€ ë°ì´í„° ì¡°íšŒ (Excel ì§ì ‘ ì ‘ê·¼ X)
    - ì¶”ì¶œëœ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë³´ê³ ì„œ ìƒì„± ì—ì´ì „íŠ¸ë¥¼ ìœ„í•œ ê°€ì´ë“œë¼ì¸ ìƒì„±
    - âš ï¸ ì¡°ê±´ë¶€ ì‹¤í–‰: ì¶”ê°€ ë°ì´í„°ê°€ DBì— ì ì¬ëœ ê²½ìš°ì—ë§Œ ì‹¤í–‰
    - ë‹¤ì¤‘ ì‚¬ì—…ì¥ ë³‘ë ¬ ì²˜ë¦¬ ì§€ì› (asyncio.gather)

ì•„í‚¤í…ì²˜:
    - additional_data_loader.py: ETL (Excel â†’ DB ì ì¬) â† ë³„ë„ íŠ¸ë¦¬ê±°ë¡œ ì‹¤í–‰
    - additional_data_agent.py: ë¶„ì„ (DB ì¡°íšŒë§Œ â†’ LLM â†’ ê°€ì´ë“œë¼ì¸) â† Node 0ì—ì„œ í˜¸ì¶œ

ë³€ê²½ ì´ë ¥:
    - 2025-12-14: v01 - ì´ˆê¸° ìƒì„± (TCFD Report v2 Refactoring)
    - 2025-12-15: v02 - ë‹¤ì¤‘ ì‚¬ì—…ì¥ ë°°ì¹˜ ì²˜ë¦¬ í™•ì¸, TCFD Report v2.1 ëŒ€ì‘
    - 2025-12-15: v03 - ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ (asyncio.gather, ì „ì²´ async ì „í™˜)
    - 2025-12-16: v04 - ETL ë¶„ë¦¬ (AdditionalDataLoader ì‚¬ìš©)
    - 2025-12-16: v05 - DB ì¡°íšŒë§Œ í•˜ë„ë¡ ìˆ˜ì • (Excel ì§ì ‘ ì ‘ê·¼ X)
    - 2025-12-16: v06 - ì˜ì–´ í”„ë¡¬í”„íŠ¸ + í•œê¸€ ì¶œë ¥ (í† í° íš¨ìœ¨í™”)
    - 2025-12-16: v07 - 2ì„¹ì…˜ JSON ì¶œë ¥ (data_summary + report_guidelines), BC Agentì™€ êµ¬ì¡° í†µì¼
    - 2025-12-17: v08 - Concrete Quantitative Data: key_data_pointsì— êµ¬ì²´ì  ìˆ˜ì¹˜, ë‹¨ìœ„, ê¸°ê°„ í¬í•¨ ê°•ì œ
                        (e.g., "2024 electricity: 500 kWh/month (+10% YoY)" instead of "High electricity usage")
'''

from typing import Dict, Any, List, Optional
import logging
from datetime import datetime
import json
import asyncio

# AdditionalDataLoader ì„í¬íŠ¸ (ETL ë‹´ë‹¹)
try:
    from .additional_data_loader import AdditionalDataLoader
except ImportError:
    AdditionalDataLoader = None
    print("âš ï¸ AdditionalDataLoaderë¥¼ ì„í¬íŠ¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

logger = logging.getLogger(__name__)


class AdditionalDataAgent:
    """
    ì¶”ê°€ ë°ì´í„° ë¶„ì„ ì—ì´ì „íŠ¸ (DB â†’ LLM Guideline)
    â†’ v05: DB ì¡°íšŒë§Œ (Excel ì§ì ‘ ì ‘ê·¼ X)

    ì…ë ¥:
        - site_ids: List[str] (ë¶„ì„ ëŒ€ìƒ ì‚¬ì—…ì¥ UUID ë¦¬ìŠ¤íŠ¸)

    ì¶œë ¥:
        - site_specific_guidelines: Dict[str, Dict] (ì‚¬ì—…ì¥ë³„ ê°€ì´ë“œë¼ì¸)
        - summary: str (ì „ì²´ ìš”ì•½)

    ì•„í‚¤í…ì²˜:
        - AdditionalDataLoader: ETL (Excel â†’ DB) - ë³„ë„ íŠ¸ë¦¬ê±°
        - AdditionalDataAgent: ë¶„ì„ (DB ì¡°íšŒë§Œ â†’ LLM â†’ ê°€ì´ë“œë¼ì¸) - Node 0 í˜¸ì¶œ
    """

    def __init__(self, llm_client=None, db_url: Optional[str] = None, db_manager=None):
        """
        ì´ˆê¸°í™”
        :param llm_client: LLM í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ (í…ìŠ¤íŠ¸ ìƒì„±ìš©)
        :param db_url: Datawarehouse DB URL (site_additional_data í…Œì´ë¸” ì ‘ê·¼ìš©) - DEPRECATED
        :param db_manager: DatabaseManager ì¸ìŠ¤í„´ìŠ¤ (ê¶Œì¥)
        """
        self.logger = logger
        self.llm_client = llm_client

        # AdditionalDataLoader ì´ˆê¸°í™” (DB ì¡°íšŒìš©)
        if AdditionalDataLoader:
            try:
                self.data_loader = AdditionalDataLoader(db_url=db_url, db_manager=db_manager)
                self.logger.info("AdditionalDataLoader ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                self.logger.error(f"AdditionalDataLoader ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.data_loader = None
        else:
            self.data_loader = None

        self.logger.info("AdditionalDataAgent ì´ˆê¸°í™” ì™„ë£Œ")

    async def analyze_from_db(self, site_ids: List[str]) -> Dict[str, Any]:
        """
        DBì—ì„œ ì¶”ê°€ ë°ì´í„° ì¡°íšŒ ë° ê°€ì´ë“œë¼ì¸ ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬)
        â†’ Node 0ì—ì„œ í˜¸ì¶œí•˜ëŠ” ì£¼ ë©”ì„œë“œ

        :param site_ids: ë¶„ì„ ëŒ€ìƒ ì‚¬ì—…ì¥ UUID ë¦¬ìŠ¤íŠ¸
        :return: ë¶„ì„ ê²°ê³¼ (ì‚¬ì—…ì¥ë³„ ê°€ì´ë“œë¼ì¸ + ì „ì²´ ìš”ì•½)
        """
        self.logger.info(f"ì¶”ê°€ ë°ì´í„° ë¶„ì„ ì‹œì‘ (DB ì¡°íšŒ): {len(site_ids)}ê°œ ì‚¬ì—…ì¥")

        try:
            # 1. ê° ì‚¬ì—…ì¥ë³„ë¡œ DBì—ì„œ ì¶”ê°€ ë°ì´í„° ì¡°íšŒ
            site_data = {}
            for site_id in site_ids:
                if self.data_loader:
                    data = self.data_loader.fetch_all_for_site(site_id)
                    site_data[site_id] = data
                else:
                    self.logger.warning(f"DataLoader ì—†ìŒ, ì‚¬ì—…ì¥ {site_id} ë¹ˆ ë°ì´í„°")
                    site_data[site_id] = {}

            # 2. ê° ì‚¬ì—…ì¥ë³„ ê°€ì´ë“œë¼ì¸ ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬)
            tasks = [
                self._generate_site_guideline(site_id, data)
                for site_id, data in site_data.items()
            ]

            self.logger.info(f"ğŸ”„ {len(tasks)}ê°œ ì‚¬ì—…ì¥ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘")
            guidelines_list = await asyncio.gather(*tasks)

            # ê²°ê³¼ë¥¼ dictë¡œ ë³€í™˜
            site_specific_guidelines = {
                site_id: guideline
                for site_id, guideline in zip(site_data.keys(), guidelines_list)
            }
            self.logger.info(f"âœ… {len(site_specific_guidelines)}ê°œ ì‚¬ì—…ì¥ ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ")

            # 3. ì „ì²´ ìš”ì•½ (Optional)
            summary = await self._generate_summary(site_specific_guidelines)

            result = {
                "meta": {
                    "analyzed_at": datetime.now().isoformat(),
                    "source": "database",
                    "site_count": len(site_specific_guidelines)
                },
                "site_specific_guidelines": site_specific_guidelines,
                "summary": summary,
                "status": "completed"
            }

            self.logger.info("ì¶”ê°€ ë°ì´í„° ë¶„ì„ ì™„ë£Œ (DB ì¡°íšŒ)")
            return result

        except Exception as e:
            self.logger.error(f"ì¶”ê°€ ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "meta": {
                    "analyzed_at": datetime.now().isoformat(),
                    "source": "database",
                    "error": str(e)
                },
                "site_specific_guidelines": {},
                "summary": "",
                "status": "failed"
            }

    async def analyze(self, excel_file: str = None, site_ids: List = None) -> Dict[str, Any]:
        """
        ì¶”ê°€ ë°ì´í„° ë¶„ì„ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)

        âš ï¸ DEPRECATED: ìƒˆë¡œìš´ ì½”ë“œì—ì„œëŠ” analyze_from_db() ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.

        :param excel_file: Excel íŒŒì¼ ê²½ë¡œ (ë¯¸ì‚¬ìš©, í˜¸í™˜ì„± ìœ ì§€)
        :param site_ids: ë¶„ì„ ëŒ€ìƒ ì‚¬ì—…ì¥ ID ë¦¬ìŠ¤íŠ¸ (str UUID ë˜ëŠ” int)
        :return: ë¶„ì„ ê²°ê³¼ (ì‚¬ì—…ì¥ë³„ ê°€ì´ë“œë¼ì¸ + ì „ì²´ ìš”ì•½)
        """
        self.logger.warning("analyze()ëŠ” deprecatedì…ë‹ˆë‹¤. analyze_from_db()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")

        # site_idsë¥¼ strë¡œ ë³€í™˜
        if site_ids:
            site_ids_str = [str(sid) for sid in site_ids]
        else:
            site_ids_str = []

        # DB ì¡°íšŒ ë©”ì„œë“œë¡œ ìœ„ì„
        return await self.analyze_from_db(site_ids_str)

    async def _generate_site_guideline(self, site_id: int, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì‚¬ì—…ì¥ë³„ ê°€ì´ë“œë¼ì¸ ìƒì„± (LLM í™œìš© - ë¹„ë™ê¸°)
        v07: JSON ì¶œë ¥ (data_summary + report_guidelines)
        """
        if not data:
            return {
                "site_id": site_id,
                "agent_guidelines": {
                    "data_summary": {
                        "one_liner": "ì¶”ê°€ ë°ì´í„° ì—†ìŒ",
                        "key_data_points": [],
                        "data_availability": "Low"
                    },
                    "report_guidelines": {
                        "scenario_analysis_focus": "ë°ì´í„° ë¶€ì¡±",
                        "impact_analysis_focus": "ë°ì´í„° ë¶€ì¡±",
                        "mitigation_focus": "ë°ì´í„° ë¶€ì¡±"
                    }
                }
            }

        # LLM ì‚¬ìš©
        if self.llm_client:
            try:
                prompt = self._build_prompt(site_id, data)

                # ë¹„ë™ê¸° LLM í˜¸ì¶œ
                if hasattr(self.llm_client, 'ainvoke'):
                    response = await self.llm_client.ainvoke(prompt)
                else:
                    # Fallback to sync invoke
                    response = self.llm_client.invoke(prompt)

                # AIMessageì—ì„œ content ì¶”ì¶œ
                response_text = response.content if hasattr(response, 'content') else str(response)

                # JSON íŒŒì‹±
                parsed_guidelines = self._parse_json_response(response_text)
                if parsed_guidelines:
                    return {
                        "site_id": site_id,
                        "agent_guidelines": parsed_guidelines
                    }

                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ fallback
                self.logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨ (ì‚¬ì—…ì¥ {site_id})")
                return self._generate_fallback_guideline(site_id, data)

            except Exception as e:
                self.logger.error(f"LLM ê°€ì´ë“œë¼ì¸ ìƒì„± ì‹¤íŒ¨ (ì‚¬ì—…ì¥ {site_id}): {e}")
                return self._generate_fallback_guideline(site_id, data)

        return self._generate_fallback_guideline(site_id, data)

    def _parse_json_response(self, response_text: str) -> Optional[Dict[str, Any]]:
        """
        LLM ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ ë° íŒŒì‹± (BC Agentì™€ ë™ì¼í•œ ë¡œì§)
        """
        import re
        import json

        # 1. ìˆœìˆ˜ JSONì¸ ê²½ìš°
        try:
            clean_text = response_text.strip()
            if clean_text.startswith('{'):
                return json.loads(clean_text)
        except json.JSONDecodeError:
            pass

        # 2. ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ë‚´ JSON ì¶”ì¶œ
        try:
            json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response_text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(1))
        except (json.JSONDecodeError, AttributeError):
            pass

        # 3. ì¤‘ê´„í˜¸ ê¸°ë°˜ ì¶”ì¶œ ì‹œë„
        try:
            start_idx = response_text.find('{')
            end_idx = response_text.rfind('}')
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                json_str = response_text[start_idx:end_idx + 1]
                return json.loads(json_str)
        except json.JSONDecodeError:
            pass

        return None

    def _generate_fallback_guideline(self, site_id: int, data: Any) -> Dict[str, Any]:
        """LLM ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ JSON ê°€ì´ë“œë¼ì¸ ìƒì„± (v07: 2ì„¹ì…˜ êµ¬ì¡°)"""
        key_data_points = []
        one_liner = "ì¶”ê°€ ë°ì´í„° ì—†ìŒ"
        availability = "Low"

        if data:
            # dataê°€ dictì¸ ê²½ìš° (ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„°)
            if isinstance(data, dict):
                categories = list(data.keys())
                total_files = sum(len(v) if isinstance(v, list) else 1 for v in data.values())

                if categories:
                    one_liner = f"{len(categories)}ê°œ ì¹´í…Œê³ ë¦¬ì˜ ì¶”ê°€ ë°ì´í„° ({total_files}ê°œ íŒŒì¼)"
                    key_data_points = [f"{cat}: {len(data[cat])}ê°œ íŒŒì¼" if isinstance(data[cat], list) else f"{cat}: 1ê°œ" for cat in categories[:3]]
                    availability = "High" if total_files > 5 else "Medium"

            # dataê°€ listì¸ ê²½ìš° (ì‹œê³„ì—´ ë°ì´í„°)
            elif isinstance(data, list):
                if len(data) > 0:
                    first_row = data[0]
                    columns = list(first_row.keys()) if isinstance(first_row, dict) else []
                    one_liner = f"ì‹œê³„ì—´ ë°ì´í„° {len(data)}ê°œ ë ˆì½”ë“œ"
                    key_data_points = [f"{col} ë°ì´í„° í¬í•¨" for col in columns[:3]]
                    availability = "High" if len(data) > 10 else "Medium"

        return {
            "site_id": site_id,
            "agent_guidelines": {
                "data_summary": {
                    "one_liner": one_liner,
                    "key_data_points": key_data_points,
                    "data_availability": availability
                },
                "report_guidelines": {
                    "scenario_analysis_focus": "ì œê³µëœ ì¶”ê°€ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„ì— ë°˜ì˜",
                    "impact_analysis_focus": "ì¶”ê°€ ë°ì´í„°ì˜ ì •ëŸ‰ì  ì •ë³´ë¥¼ ì˜í–¥ ë¶„ì„ì— í™œìš©",
                    "mitigation_focus": "ë°ì´í„° ê¸°ë°˜ ëŒ€ì‘ ì „ëµ ìˆ˜ë¦½"
                }
            }
        }

    def _build_prompt(self, site_id: int, data: Dict[str, Any]) -> str:
        """
        LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì¶”ê°€ ë°ì´í„° â†’ ê°€ì´ë“œë¼ì¸ ë³€í™˜)
        v07 ì—…ë°ì´íŠ¸: 2ì„¹ì…˜ ê°„ì†Œí™” (data_summary + report_guidelines)
        """
        # datetime ì§ë ¬í™” í•¸ë“¤ëŸ¬
        def json_serializer(obj):
            if hasattr(obj, 'isoformat'):
                return obj.isoformat()
            raise TypeError(f"Object of type {type(obj)} is not JSON serializable")

        # ë°ì´í„° ìš”ì•½ (í† í° ì œí•œ ë•Œë¬¸ì— ì „ì²´ ë°ì´í„° ëŒ€ì‹  ìš”ì•½ë§Œ)
        summarized_data = self._summarize_data_for_prompt(data)
        data_json = json.dumps(summarized_data, indent=2, ensure_ascii=False, default=json_serializer)

        prompt = f"""<ROLE>
You are a TCFD report expert. Analyze additional site data and generate **concise guidelines** for TCFD report agents.
</ROLE>

<ADDITIONAL_DATA>
Site ID: {site_id}

{data_json}
</ADDITIONAL_DATA>

<CRITICAL_REQUIREMENTS>
**CONCRETE QUANTITATIVE DATA IS ESSENTIAL**

When extracting key_data_points, you MUST provide:
1. **Specific numerical values** (e.g., "500 kWh", "120 mÂ³", "5 billion KRW")
2. **Time periods** (e.g., "2024", "Q3 2024", "year-over-year")
3. **Comparisons/trends** (e.g., "+10% YoY", "peak season", "below threshold")
4. **Unit-specific details** (e.g., "per month", "annually", "during summer")

âŒ WRONG (too abstract):
- "High electricity usage"
- "Insurance data available"
- "Energy consumption tracked"

âœ… CORRECT (concrete quantitative):
- "2024 electricity: 500 kWh/month (+10% YoY)"
- "Gas consumption: 120 mÂ³ (peak in Jul-Aug)"
- "No fire insurance (building value: 5B KRW)"
- "Water usage: 200 tons/month (2024 avg)"
- "Renewable energy: 30% of total consumption"

**EXTRACT NUMBERS FROM THE DATA** - Don't describe, quantify!
</CRITICAL_REQUIREMENTS>

<OUTPUT_FORMAT>
Generate JSON with 3 sections (Hybrid Structure):

{{
  "data_summary": {{
    "one_liner": "Comprehensive 1-2 sentence summary (100-150 chars) with PRIMARY quantitative insight. Example: '5 data categories covering 2024 energy (15,000 kWh), insurance (5B KRW gap), and water usage (200 tons/month)'",
    "key_data_points": [
      "MUST provide 5-7 data points, each 60-100 chars with SPECIFIC numbers",
      "Example: '2024 electricity: 500 kWh/month (+10% YoY, peak in summer)'",
      "Example: 'Insurance gap: 2B KRW (building value 10B, coverage 8B)'",
      "Example: 'Gas usage: 120 mÂ³/month (heating season: +40%)'",
      "Example: 'Renewable energy: 30% of total (solar panels installed 2023)'",
      "Example: 'Water consumption: 200 tons/month (industrial process: 80%)'",
      "Example: 'Equipment age: HVAC 12 years, electrical 8 years (maintenance due)'"
    ],
    "data_availability": "High/Medium/Low with specific justification"
  }},

  "report_guidelines": {{
    "scenario_analysis_focus": "4-6 sentences (250-350 chars) explaining HOW quantitative data informs scenario analysis: which climate scenarios are most relevant given the site's energy profile, operational patterns, and geographic risks. Include specific thresholds and trigger points.",
    "impact_analysis_focus": "4-6 sentences (250-350 chars) detailing HOW quantitative data feeds impact calculations: financial implications (costs, insurance gaps), operational disruptions (downtime estimates), and asset depreciation risks. Include specific monetary figures when available.",
    "mitigation_focus": "4-6 sentences (250-350 chars) describing HOW quantitative data shapes mitigation priorities: energy efficiency targets, infrastructure upgrades needed, insurance coverage gaps to address. Include ROI considerations and timeline recommendations."
  }},

  "detailed_context": "Comprehensive natural narrative (6-10 paragraphs, 1200-1800 chars). MUST include ALL of the following in detail: (1) Data completeness assessment - what categories are available and what's missing, (2) Energy consumption analysis with specific figures (kWh, mÂ³, trends), (3) Insurance coverage analysis with specific values and gaps, (4) Operational patterns that affect climate vulnerability, (5) Key quantitative insights for each TCFD pillar (Governance, Strategy, Risk Management, Metrics), (6) Data-driven recommendations for downstream report generation. Each paragraph should provide actionable, quantitative insights."
}}

**OUTPUT LANGUAGE: Korean (í•œê¸€).** All text must be in Korean (í•œê¸€) for local stakeholders.

**CRITICAL LENGTH REQUIREMENTS:**
- Total output: 1500-2500 characters MINIMUM
- detailed_context: 1200-1800 characters (most critical section)
- key_data_points: Each item should be 60-100 characters with SPECIFIC numbers, units, and time periods
- Each report_guidelines field: 250-350 characters with concrete, actionable guidance

âš ï¸ OUTPUT THAT IS SHORTER THAN 1500 CHARACTERS WILL BE REJECTED. Provide comprehensive, quantitative analysis.

**PRIORITY:** Focus on extracting concrete numbers, units, time periods, and trends.

Output pure JSON only. No markdown.
</OUTPUT_FORMAT>
"""
        return prompt

    def _summarize_data_for_prompt(self, data: Dict[str, Any], max_rows_per_sheet: int = 20) -> Dict[str, Any]:
        """
        LLM í† í° ì œí•œì„ ìœ„í•´ ë°ì´í„° ìš”ì•½ (ì „ì²´ ë¤í”„ ëŒ€ì‹  ìƒ˜í”Œë§Œ)

        Args:
            data: ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° (fetch_all_for_site ê²°ê³¼)
            max_rows_per_sheet: ì‹œíŠ¸ë‹¹ ìµœëŒ€ í–‰ ìˆ˜

        Returns:
            ìš”ì•½ëœ ë°ì´í„° (ë©”íƒ€ ì •ë³´ + ìƒ˜í”Œ í–‰)
        """
        summarized = {}

        for category, items in data.items():
            summarized[category] = []

            for item in items:
                file_info = {
                    "file_name": item.get("file_name", "unknown"),
                    "category": category,
                    "uploaded_at": str(item.get("uploaded_at", "")),
                }

                # structured_dataì—ì„œ ìƒ˜í”Œë§Œ ì¶”ì¶œ
                structured = item.get("structured_data", {})
                if isinstance(structured, dict):
                    sheets_summary = []
                    for sheet in structured.get("sheets", []):
                        sheet_name = sheet.get("name", "Sheet")
                        row_count = sheet.get("row_count", 0)
                        content = sheet.get("content", "")

                        # ì²˜ìŒ Ní–‰ë§Œ ì¶”ì¶œ
                        lines = content.split("\n")[:max_rows_per_sheet]
                        sample_content = "\n".join(lines)

                        sheets_summary.append({
                            "sheet_name": sheet_name,
                            "total_rows": row_count,
                            "sample_rows": len(lines),
                            "sample_content": sample_content
                        })

                    file_info["sheets"] = sheets_summary
                else:
                    file_info["data_preview"] = str(structured)[:2000]

                summarized[category].append(file_info)

        return summarized

    async def _generate_summary(self, site_specific_guidelines: Dict[int, Dict[str, Any]]) -> str:
        """
        ì „ì²´ ì‚¬ì—…ì¥ ê°€ì´ë“œë¼ì¸ ìš”ì•½ (ë¹„ë™ê¸°)
        """
        if not site_specific_guidelines:
            return "ì¶”ê°€ ë°ì´í„° ì—†ìŒ"

        summary = f"## ì¶”ê°€ ë°ì´í„° ì „ì²´ ìš”ì•½\n\n"
        summary += f"ì´ {len(site_specific_guidelines)}ê°œ ì‚¬ì—…ì¥ì— ëŒ€í•œ ì¶”ê°€ ë°ì´í„°ê°€ ì œê³µë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"

        # ì‚¬ì—…ì¥ë³„ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ìˆ˜ ì§‘ê³„
        total_insights = sum(len(g.get('key_insights', [])) for g in site_specific_guidelines.values())
        summary += f"ì´ {total_insights}ê°œì˜ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ê°€ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.\n"

        return summary
